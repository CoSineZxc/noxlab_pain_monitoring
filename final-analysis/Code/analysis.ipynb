{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy.fft import fft, ifft\n",
    "from scipy.signal import periodogram\n",
    "import ordpy \n",
    "import statsmodels.api as sm\n",
    "from scipy.signal import butter,filtfilt\n",
    "from matplotlib.pyplot import figure\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from statsmodels.tsa.stattools import adfuller, pacf\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all the data files and its path\n",
    "onlyfiles = [f for f in listdir('../Data/data_file') if isfile(join('../Data/data_file', f))]\n",
    "for i in range(len(onlyfiles)):\n",
    "    path = '../Data/data_file/' + onlyfiles[i]\n",
    "files = path\n",
    "files\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Functions\n",
    "For windowing, I realise the reaction time is always the longest in the first trial. So I think we could window more (start from 6 seconds) for the first trial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def window_value(rating, time): #use values instead of timing to window\n",
    "    rating_windowed_list = []\n",
    "    time_windowed_list = []\n",
    "    failed_index_list = [] #capture error in data logging\n",
    "    #window the trials by finding the first time that they have changed their rating\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating) > 1: #catch the whole entry being NaN\n",
    "            #catch nan, catch hitting middle of confidence slider, catch ending time\n",
    "            #i.e. this condidtion is it taps on the middle of the confidence slider\n",
    "            if len(rating[i]) > 1 and time[i][0] < 0.1 and time[i][-1] > 10:\n",
    "                res_unchanged = next(x for x, val in enumerate(rating[i]) if val != rating[i][0])\n",
    "                rating_windowed = rating[i][res_unchanged:len(rating[i])]\n",
    "                time_windowed = time[i][res_unchanged:len(rating[i])]\n",
    "                #replace trials with error in data logging with nan\n",
    "                #for trials that has a very long reaction time (about 10 seconds), filter it away\n",
    "                if time_windowed[0] < 10 and len(time_windowed) > 100:\n",
    "                    rating_windowed_list.append(rating_windowed)\n",
    "                    time_windowed_list.append(time_windowed)\n",
    "                else:\n",
    "                    rating_windowed_list.append([np.NaN])\n",
    "                    time_windowed_list.append([np.NaN])\n",
    "                    failed_index_list.append(i)\n",
    "            #condition: no nan but tap on the slider (0.1s being the reaction time)\n",
    "            elif len(rating[i]) > 1 and time[i][0] > 0.1 and time[i][-1] > 10:\n",
    "                rating_windowed = rating[i] #don't window as it starts recording only when click the slider\n",
    "                time_windowed = time[i]\n",
    "                if time_windowed[0] < 10 and len(time_windowed) > 100:\n",
    "                    rating_windowed_list.append(rating_windowed)\n",
    "                    time_windowed_list.append(time_windowed)\n",
    "                else:\n",
    "                    rating_windowed_list.append([np.NaN])\n",
    "                    time_windowed_list.append([np.NaN])\n",
    "                    failed_index_list.append(i)\n",
    "            else:\n",
    "                rating_windowed_list.append([np.NaN])\n",
    "                time_windowed_list.append([np.NaN])\n",
    "                failed_index_list.append(i)\n",
    "        else:\n",
    "            rating_windowed_list.append([np.NaN])\n",
    "            time_windowed_list.append([np.NaN])\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    return rating_windowed_list, time_windowed_list, failed_index_list\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def exponential_smoothing(rating, alpha):\n",
    "    lowpass_list = [] #list of all 30 trials\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating.iloc[i]) > 1:\n",
    "            result = [rating.iloc[i][0]] # first value is same as series\n",
    "            for n in range(1, len(rating.iloc[i])):\n",
    "                result.append(alpha * rating.iloc[i][n] + (1 - alpha) * result[n-1])\n",
    "            lowpass_list.append(result)\n",
    "        else:\n",
    "            lowpass_list.append([np.nan])\n",
    "    return lowpass_list\n",
    "\n",
    "\n",
    "def downsample(rating, time):\n",
    "    rating_downsample_list = []\n",
    "    time_downsample_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            original_timing = time.iloc[i]\n",
    "            original_rating = rating[i]\n",
    "            flinear = interpolate.interp1d(original_timing, original_rating)\n",
    "            new_timing = np.arange(round_up(original_timing[0],1),round_down(original_timing[-1],1),0.05)\n",
    "            ylinear = flinear(new_timing)\n",
    "            rating_downsample_list.append(ylinear)\n",
    "            time_downsample_list.append(new_timing)\n",
    "        else:\n",
    "            rating_downsample_list.append([np.nan])\n",
    "            time_downsample_list.append([np.nan])\n",
    "    return rating_downsample_list, time_downsample_list\n",
    "\n",
    "def round_up(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.ceil(n * multiplier) / multiplier\n",
    "def round_down(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.floor(n * multiplier) / multiplier\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean, mean square, root mean square, variance, standard deviation\n",
    "def mean(rating):\n",
    "    mean_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            mean_list.append(np.mean(rating[i]))\n",
    "        else:\n",
    "            mean_list.append(np.NaN)\n",
    "    return mean_list\n",
    "\n",
    "def mean_square(rating, time): #strength/average power of the signal\n",
    "    mean_square_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            mean_square_list.append(np.sum(np.array(rating[i]) ** 2)/(max(time[i]) - min(time[i])))\n",
    "        else:\n",
    "            mean_square_list.append(np.NaN)\n",
    "    return mean_square_list\n",
    "\n",
    "def rms(mean_square_list):\n",
    "    return np.sqrt(mean_square_list)\n",
    "\n",
    "def variance(rating,time):\n",
    "    variance_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            diff_mean_square_sum = np.sum((rating[i] - np.mean(rating[i]))**2)\n",
    "            variance = diff_mean_square_sum/(max(time[i]) - min(time[i]))\n",
    "            variance_list.append(variance)\n",
    "        else:\n",
    "            variance_list.append(np.NaN)\n",
    "    return variance_list\n",
    "\n",
    "def std(variance_list):\n",
    "    return np.sqrt(variance_list)\n",
    "  \n",
    "\n",
    "\n",
    "#frequency np.fft\n",
    "\n",
    "def fft_data(rating, time):\n",
    "    fft_weights_list = []\n",
    "    fft_freq_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            fft_weights = fft(rating[i])\n",
    "            N = len(fft_weights)\n",
    "            n = np.arange(N)\n",
    "            T = time[i][-1]-time[i][0]\n",
    "            fft_freq = n/T\n",
    "            fft_weights_list.append(fft_weights)\n",
    "            fft_freq_list.append(fft_freq)\n",
    "        else:\n",
    "            fft_weights_list.append([np.NaN])\n",
    "            fft_freq_list.append([np.NaN])\n",
    "    return fft_weights_list, fft_freq_list\n",
    "\n",
    "\n",
    "#power-spectrum\n",
    "def power_spectrum(rating,time):\n",
    "    psd_freq_list = []\n",
    "    psd_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            N = len(rating[i])\n",
    "            T = time[i][-1]\n",
    "            f, S = periodogram(rating[i], N/T, scaling = 'density')\n",
    "            psd_freq_list.append(f)\n",
    "            psd_list.append(S)\n",
    "        else:\n",
    "            psd_freq_list.append([np.NaN])\n",
    "            psd_list.append([np.NaN])\n",
    "    return psd_freq_list, psd_list\n",
    "\n",
    "\n",
    "#permutation entropy - complexity\n",
    "def permutation_entropy(rating):\n",
    "    entropy_list = []\n",
    "    stat_complexity_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            entropy, stat_complexity = ordpy.complexity_entropy(rating[i])\n",
    "            entropy_list.append(entropy)\n",
    "            stat_complexity_list.append(stat_complexity)\n",
    "        else:\n",
    "            entropy_list.append(np.NaN)\n",
    "            stat_complexity_list.append(np.NaN)\n",
    "    return entropy_list, stat_complexity_list\n",
    "\n",
    "#autocorrelation\n",
    "def autocorrelation(rating, time):\n",
    "    acorr_list = []\n",
    "    time_lag_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            acorr = sm.tsa.acf(rating[i], nlags = int(len(rating[i])))\n",
    "            acorr_list.append(acorr)\n",
    "        else:\n",
    "            acorr_list.append([np.NaN, np.NaN, np.NaN])\n",
    "    for i in range(len(time)):\n",
    "        if len(time[i]) > 1:\n",
    "            time_lag = np.array(time[i]) - np.array(time[i][0])\n",
    "            time_lag_list.append(time_lag)\n",
    "        else:\n",
    "            time_lag_list.append([np.NaN, np.NaN, np.NaN])\n",
    "    return acorr_list, time_lag_list\n",
    "\n",
    "#check stationarity\n",
    "def adfuller_test(rating):\n",
    "    adfuller_list = []\n",
    "    stationary_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            result = adfuller(rating[i])\n",
    "            adfuller_list.append(result)\n",
    "            if result[1] <= 0.05:\n",
    "                stationary_list.append(1)\n",
    "            else:\n",
    "                stationary_list.append(0)\n",
    "        else:\n",
    "            adfuller_list.append(np.nan)\n",
    "            stationary_list.append(np.nan)\n",
    "    labels = ['ADF Test Statistic','p-value','#Lags Used','#Observation Used']\n",
    "    return adfuller_list, labels, stationary_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(files)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put continuous pain data into correct form & visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    plt.scatter(data['time'].iloc[i], data['rating'].iloc[i], s = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_all = []\n",
    "for i in range(0,len(data),3):\n",
    "    rating_all.append(data['rating'].iloc[i:i+3])\n",
    "timing_all = []\n",
    "for i in range(0,len(data),3):\n",
    "    timing_all.append(data['time'].iloc[i:i+3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data\n",
    "## exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowpass_list_all = []\n",
    "for i in range(len(participant_list)):\n",
    "    lowpass_list_all.append(exponential_smoothing(rating_all[i], alpha = 0.1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the timing method\n",
    "rating_downsample_list_all = []\n",
    "time_downsample_list_all = []\n",
    "\n",
    "\n",
    "\n",
    "for j in tqdm(range(len(lowpass_list_all))):\n",
    "    rating_downsample_list, time_downsample_list = downsample(lowpass_list_all[j], timing_all[j])\n",
    "    rating_downsample_list_all.append(rating_downsample_list)\n",
    "    time_downsample_list_all.append(time_downsample_list)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rating_downsample_list_all)):\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(timing_all[i].iloc[0],rating_all[i].iloc[0], s = 0.05)\n",
    "    plt.scatter(timing_all[i].iloc[1],rating_all[i].iloc[1], s = 0.05)\n",
    "    plt.scatter(timing_all[i].iloc[2],rating_all[i].iloc[2], s = 0.05)\n",
    "    plt.xlabel('time (s)')\n",
    "    plt.ylabel('rating')\n",
    "    plt.title('original')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(time_downsample_list_all[i][0],rating_downsample_list_all[i][0], s = 0.05)\n",
    "    plt.scatter(time_downsample_list_all[i][1],rating_downsample_list_all[i][1], s = 0.05)\n",
    "    plt.scatter(time_downsample_list_all[i][2],rating_downsample_list_all[i][2], s = 0.05)\n",
    "    plt.xlabel('time (s)')\n",
    "    plt.ylabel('rating')\n",
    "    plt.title('Downsampled to 20 Hz')\n",
    "    plt.suptitle(f'participant {participant_list[i]}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "## common statistical parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list_all = []\n",
    "mean_square_list_all = []\n",
    "rms_list_all = []\n",
    "variance_list_all = []\n",
    "std_list_all = []\n",
    "\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "    mean_list = mean(rating_downsample_list_all[j])\n",
    "    mean_square_list = mean_square(rating_downsample_list_all[j], time_downsample_list_all[j])\n",
    "    rms_list = rms(mean_square_list)\n",
    "    variance_list= variance(rating_downsample_list_all[j], time_downsample_list_all[j])\n",
    "    std_list = std(variance_list)\n",
    "    mean_list_all.append(mean_list)\n",
    "    mean_square_list_all.append(mean_square_list)\n",
    "    rms_list_all.append(rms_list)\n",
    "    variance_list_all.append(variance_list)\n",
    "    std_list_all.append(std_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3]\n",
    "for i in range(len(mean_list_all)):\n",
    "    plt.figure()\n",
    "    plt.scatter(x, mean_list_all[i])\n",
    "    plt.ylim(0,10)\n",
    "    plt.xlabel('trial')\n",
    "    plt.ylabel('mean value')\n",
    "    plt.title(f'Mean value of {participant_list[i]}')\n",
    "    plt.xticks([1,2,3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_overall = []\n",
    "for i in range(len(mean_list_all)):\n",
    "    mean_overall.append(np.mean(mean_list_all[i]))\n",
    "plt.hist(mean_overall, bins = 10)\n",
    "plt.xlabel('rating')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Disbtribution of all the mean values of all participants in Day 1')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3]\n",
    "for i in range(len(mean_square_list_all)):\n",
    "    plt.figure()\n",
    "    plt.scatter(x, mean_square_list_all[i])\n",
    "    plt.xlabel('trial')\n",
    "    plt.ylabel('mean square value')\n",
    "    plt.title(f'Mean square value of {participant_list[i]}')\n",
    "    plt.xticks([1,2,3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3]\n",
    "for i in range(len(variance_list_all)):\n",
    "    plt.figure()\n",
    "    plt.scatter(x, variance_list_all[i])\n",
    "    plt.xlabel('trial')\n",
    "    plt.ylabel('variance value')\n",
    "    plt.title(f'variance value of {participant_list[i]}')\n",
    "    plt.xticks([1,2,3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_all(rating):\n",
    "    rating_collected = np.hstack([rating[0], rating[1], rating[2]])\n",
    "    return np.nanvar(rating_collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_combined_list_all = []\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "    var_combined_list = var_all(rating_downsample_list_all[j])\n",
    "    var_combined_list_all.append(var_combined_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(var_combined_list_all, bins = 20)\n",
    "plt.xlabel('variance')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('variance distribution of participants on Day 1')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency\n",
    "### FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(time_downsample_list_all[j][0][-1] - time_downsample_list_all[j][0][0])/len(time_downsample_list_all[j][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_weights_list_all = []\n",
    "fft_freq_list_all = []\n",
    "\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "   \n",
    "        plt.figure()\n",
    "        fft_weights_list, fft_freq_list = fft_data(rating_downsample_list_all[j], time_downsample_list_all[j])\n",
    "        fft_weights_list_all.append(fft_weights_list)\n",
    "        fft_freq_list_all.append(fft_freq_list)\n",
    "        for i in range(len(rating_downsample_list_all[j])):\n",
    "            plt.plot(fft_freq_list[i], np.abs(fft_weights_list[i]))\n",
    "        plt.xlim(0,0.15)\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Weight')\n",
    "        plt.title(f'Frequency weights of the signal with FFT for \\n participant {participant_list[j]}')\n",
    "        plt.yscale('log')\n",
    "        plt.ylim(0.1,10**4.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fft_freq_list_all)):\n",
    "    for j in range(3):\n",
    "        plt.plot(fft_freq_list_all[i][j], np.abs(fft_weights_list_all[i][j]))\n",
    "plt.xlim(0,0.15)\n",
    "plt.ylim(0.1,10**4.5)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('FFT of all the participants')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psd_freq_list_all = []\n",
    "psd_list_all=[]\n",
    "\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "    plt.figure()\n",
    "    psd_freq_list, psd_list = power_spectrum(rating_downsample_list_all[j], time_downsample_list_all[j])\n",
    "    psd_freq_list_all.append(psd_freq_list)\n",
    "    psd_list_all.append(psd_list)\n",
    "    for i in range(len(lowpass_list_all[j])):\n",
    "        plt.plot(psd_freq_list[i], np.abs(psd_list[i]))\n",
    "    plt.xlim(0,0.15)\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Power Spectral Density')\n",
    "    plt.title(f'Power Spectral Density of the pain signal for \\n participant {participant_list[j]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### power spectrum all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(psd_freq_list_all)):\n",
    "    for j in range(3):\n",
    "        plt.plot(psd_freq_list_all[i][j], np.abs(psd_list_all[i][j]))\n",
    "plt.xlim(0,0.15)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Power spectrum of all the participants')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_list_all = []\n",
    "stat_complexity_list_all = []\n",
    "for i in range(len(rating_downsample_list_all)):\n",
    "    plt.figure()\n",
    "    entropy_list, stat_complexity_list = permutation_entropy(rating_downsample_list_all[i])\n",
    "    entropy_list_all.append(entropy_list)\n",
    "    stat_complexity_list_all.append(stat_complexity_list)\n",
    "    plt.scatter(range(1,len(entropy_list)+1), entropy_list)\n",
    "    plt.title(f'Permutation entropy of the data for \\n participant {participant_list[i]}')\n",
    "    plt.xlabel('trial number')\n",
    "    plt.ylabel('permutation entropy')\n",
    "    plt.ylim(-0.1,1.1)\n",
    "    plt.xticks([1,2,3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation entropy all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(entropy_list_all)):\n",
    "    plt.scatter(range(1,len(entropy_list_all[0])+1), entropy_list_all[i])\n",
    "plt.xlabel('trial number')\n",
    "plt.ylabel('permutation entropy')\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xticks([1,2,3])\n",
    "plt.title('Permutation entropy for all participants for each trial')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_overall = []\n",
    "for i in range(len(entropy_list_all)):\n",
    "    entropy_overall.append(np.mean(entropy_list_all[i]))\n",
    "plt.hist(entropy_overall)\n",
    "plt.xlabel('permutation entropy')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Distribution of Permutation entropy for all participants')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acorr_list_all = []\n",
    "time_lag_list_all = []\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "        \n",
    "    plt.figure()\n",
    "    acorr_list, time_lag = autocorrelation(rating_downsample_list_all[j], time_downsample_list_all[j])\n",
    "    acorr_list_all.append(acorr_list)\n",
    "    time_lag_list_all.append(time_lag)\n",
    "\n",
    "    \n",
    "    for i in range(3):\n",
    "        plt.plot(time_lag[i], acorr_list[i])\n",
    "    plt.xlabel('time lag (s)')\n",
    "    plt.ylabel('autocorrelation')\n",
    "    plt.title(f'Autocorrelation function for \\n participant {participant_list[j]}')\n",
    "    plt.ylim(-1.1,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(time_lag_list_all)):\n",
    "    for j in range(3):\n",
    "        plt.plot(time_lag_list_all[i][j], acorr_list_all[i][j])\n",
    "plt.xlabel('time lag (s)')\n",
    "plt.ylabel('autocorrelation function')\n",
    "plt.title('Autocorrelation function of all the participants')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pacf(rating):\n",
    "    pacf_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1 and rating[i][-1] != rating[i][0]:\n",
    "            pacf_values = pacf(rating[i])\n",
    "            pacf_list.append(pacf_values)\n",
    "        else:\n",
    "            pacf_list.append([np.NaN, np.NaN, np.NaN])\n",
    "    return pacf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_list[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_lag_list_pacf = np.arange(0,100,0.05)\n",
    "pacf_list_all = []\n",
    "for j in tqdm(range(len(rating_downsample_list_all))):\n",
    "    plt.figure()\n",
    "    pacf_list = cal_pacf(rating_downsample_list_all[j])\n",
    "    for i in range(3):\n",
    "        plt.plot(time_lag_list_pacf[:len(pacf_list[i])],pacf_list[i])\n",
    "        plt.ylabel('PACF value')\n",
    "        plt.xlabel('time lag (s)')\n",
    "        plt.title(f'PACF of {participant_list[j]}')\n",
    "    pacf_list_all.append(pacf_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PACF of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pacf_list_all)):\n",
    "    for j in range(3):\n",
    "        plt.plot(time_lag_list_pacf[:len(pacf_list_all[i][j])], pacf_list_all[i][j])\n",
    "#plt.yscale('log')\n",
    "plt.xlabel('time lag (s)')\n",
    "plt.ylabel('PACF value')\n",
    "plt.title('PACF of all the participants')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller_list_all = []\n",
    "stationary_list_all = []\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "    adfuller_list, labels, stationary_list = adfuller_test(rating_downsample_list_all[j])\n",
    "    adfuller_list_all.append(adfuller_list)\n",
    "    stationary_list_all.append(stationary_list)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series model\n",
    "### ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.arima import auto_arima\n",
    "def calcsmape(actual, forecast):\n",
    "    return 1/len(actual) * np.sum(2 * np.abs(forecast-actual) / (np.abs(actual) + np.abs(forecast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 100\n",
    "smape_all = []\n",
    "model_all = []\n",
    "for i in tqdm(range(len(rating_downsample_list_all))):\n",
    "    smape_participant = []\n",
    "    model_participant = []\n",
    "    for j in range(3):\n",
    "        data_trial = pd.Series(rating_downsample_list_all[i][j])\n",
    "        train, test = data_trial[:-TEST_SIZE], data_trial[-TEST_SIZE:]\n",
    "\n",
    "        x_train, x_test = np.array(range(train.shape[0])), np.array(range(train.shape[0], data_trial.shape[0]))\n",
    "        train.shape, x_train.shape, test.shape, x_test.shape\n",
    "        model = auto_arima(train, start_p=1, start_q=1,\n",
    "                            test='adf',\n",
    "                            max_p=5, max_q=5,\n",
    "                            m=1,             \n",
    "                            d=1,          \n",
    "                            seasonal=False,   \n",
    "                            start_P=0, \n",
    "                            D=None, \n",
    "                            trace=True,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)\n",
    "        model_participant.append(model)\n",
    "        prediction, confint = model.predict(n_periods=TEST_SIZE, return_conf_int=True)\n",
    "        cf= pd.DataFrame(confint)\n",
    "        prediction_series = pd.Series(prediction, index = test.index)\n",
    "        fig1, ax1 = plt.subplots(1, 1, figsize=(15, 5))\n",
    "        ax1.plot(data_trial, label = 'original')\n",
    "        ax1.plot(prediction_series, label = 'forecasting')\n",
    "        ax1.fill_between(prediction_series.index,\n",
    "                        cf[0],\n",
    "                        cf[1],color='grey',alpha=.3)\n",
    "        ax1.set_ylim(0,10)\n",
    "        ax1.set_title(f'participant id {participant_list[i]}, day 1, trial {j+1}')\n",
    "        ax1.legend()\n",
    "        smape=calcsmape(test,prediction)\n",
    "        smape_participant.append(smape)\n",
    "    smape_all.append(smape_participant)\n",
    "    model_all.append(model_participant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data_trial = pd.DataFrame(data_trial)\n",
    "# Fit scalers\n",
    "#scalers = {}\n",
    "#scalers = StandardScaler().fit(data_trial.values.reshape(-1, 1))\n",
    "\n",
    "object= StandardScaler()\n",
    " \n",
    " \n",
    "# standardization \n",
    "scale = object.fit_transform(data_trial) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'PID': participant_list, 'mean': mean_list_all, 'mean overall': mean_overall, 'mean square': mean_square_list_all, 'variance': variance_list_all, 'variance overall': var_combined_list_all, 'standard deviation': std_list_all, 'FFT frequency': fft_freq_list_all, 'FFT weights': fft_weights_list_all, 'power spectrum frequency': psd_freq_list_all, 'power spectrum weights': psd_list_all, 'permutation entropy': entropy_list_all, 'permutation entropy overall': entropy_overall, 'autocorrelation': acorr_list_all, 'PACF': pacf_list_all, 'stationary': stationary_list_all, 'ARIMA model': model_all, 'SMAPE': smape_all}\n",
    "data_descriptive = pd.DataFrame(data_dict)\n",
    "data_descriptive.to_pickle('../Data/descriptive_data/DescriptiveDataARIMAmodel.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try analysis\n",
    "### Mean pain VS PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_entropy = zip(mean_overall, entropy_overall)\n",
    "x = []; y=[]\n",
    "for point in list(mean_entropy):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "x_bestfit = np.arange(0,11,0.1)\n",
    "y_bestfit = a*x_bestfit+b\n",
    "plt.plot(x_bestfit, y_bestfit, color = 'green', linestyle = '--')\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('Mean')\n",
    "plt.ylabel('permutation entropy')\n",
    "plt.xlim(0,10)\n",
    "plt.ylim(0,1)\n",
    "plt.text(6, 0.2, 'y = ' + '{:.2f}'.format(b) + ' {:.2f}'.format(a) + 'x', size=14)\n",
    "plt.title('permutation entropy against mean')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSK scores and Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all the data files and its path\n",
    "questionnaire = []\n",
    "onlyfiles = [f for f in listdir('../Data/questionnaire') if isfile(join('../Data/questionnaire', f))]\n",
    "for i in range(len(onlyfiles)):\n",
    "    path = '../Data/questionnaire/' + onlyfiles[i]\n",
    "    questionnaire.append(path)\n",
    "\n",
    "questionnaire\n",
    "for item in questionnaire:\n",
    "    df_questionnaire = pd.read_csv(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for id in participant_list:\n",
    "    index.append(np.where(df_questionnaire.PROLIFIC_PID == id)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questionnaire_index = df_questionnaire.iloc[index]\n",
    "msk_scores = df_questionnaire_index['Score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mskscores = zip(mean_overall, msk_scores)\n",
    "x = []; y=[]\n",
    "for point in list(mean_mskscores):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "x_bestfit = np.arange(0,11,0.1)\n",
    "y_bestfit = a*x_bestfit+b\n",
    "plt.plot(x_bestfit, y_bestfit, color = 'green', linestyle = '--')\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('Mean')\n",
    "plt.ylabel('MSK scores')\n",
    "plt.xlim(0,10)\n",
    "plt.ylim(0,50)\n",
    "plt.text(0.5, 10, 'y = ' + '{:.2f}'.format(b) + ' {:.2f}'.format(a) + 'x', size=14)\n",
    "plt.title('MSK scores against mean')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSK scores and Permutation Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_mskscores = zip(entropy_overall, msk_scores)\n",
    "x = []; y=[]\n",
    "for point in list(pe_mskscores):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "x_bestfit = np.arange(0,1.2,0.1)\n",
    "y_bestfit = a*x_bestfit+b\n",
    "plt.plot(x_bestfit, y_bestfit, color = 'green', linestyle = '--')\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('Permutation Entropy')\n",
    "plt.ylabel('MSK scores')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,50)\n",
    "plt.text(0.5, 10, 'y = ' + '{:.2f}'.format(b) + ' + {:.2f}'.format(a) + 'x', size=14)\n",
    "plt.title('MSK scores against permutation entropy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSK scores and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var_mskscores = zip(var_combined_list_all, msk_scores)\n",
    "x = []; y=[]\n",
    "for point in list(var_mskscores):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "x_bestfit = np.arange(0,max(var_combined_list_all)+1,0.1)\n",
    "y_bestfit = a*x_bestfit+b\n",
    "plt.plot(x_bestfit, y_bestfit, color = 'green', linestyle = '--')\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('MSK scores')\n",
    "#plt.xlim(0,1)\n",
    "plt.ylim(0,50)\n",
    "plt.text(0.5, 10, 'y = ' + '{:.2f}'.format(b) + ' + {:.2f}'.format(a) + 'x', size=14)\n",
    "plt.title('MSK scores against variance')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance and mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var_mean = zip(var_combined_list_all, mean_overall)\n",
    "x = []; y=[]\n",
    "for point in list(var_mean):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "x_bestfit = np.arange(0,max(var_combined_list_all)+1,0.1)\n",
    "y_bestfit = a*x_bestfit+b\n",
    "plt.plot(x_bestfit, y_bestfit, color = 'green', linestyle = '--')\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Mean')\n",
    "#plt.xlim(0,1)\n",
    "plt.ylim(0,50)\n",
    "plt.text(0.5, 10, 'y = ' + '{:.2f}'.format(b) + ' {:.2f}'.format(a) + 'x', size=14)\n",
    "plt.title('mean against variance')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance and permutation entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var_pe = zip(var_combined_list_all, entropy_overall)\n",
    "x = []; y=[]\n",
    "for point in list(var_pe):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "x_bestfit = np.arange(0,max(var_combined_list_all)+1,0.1)\n",
    "y_bestfit = a*x_bestfit+b\n",
    "plt.plot(x_bestfit, y_bestfit, color = 'green', linestyle = '--')\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Permutation Entropy')\n",
    "#plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.text(6, 0.2, 'y = ' + '{:.2f}'.format(b) + ' {:.2f}'.format(a) + 'x', size=14)\n",
    "plt.title('permutation entropy against variance')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the response of prediction and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all the data files and its path\n",
    "onlyfiles = [f for f in listdir('../Data/data_file') if isfile(join('../Data/data_file', f))]\n",
    "for i in range(len(onlyfiles)):\n",
    "    path = '../Data/data_file/' + onlyfiles[i]\n",
    "files = path\n",
    "files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_all = []\n",
    "confidence_all= []\n",
    "\n",
    "data = pd.read_pickle(files)\n",
    "\n",
    "prediction_all = data['prediction'].unique()\n",
    "confidence_all = data['confidence'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prediction_all, bins = 10)\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Distribution of prediction for all participants')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(confidence_all, bins = 10)\n",
    "plt.xlabel('confidence')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Distribution of confidence for all participants')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o\tEvaluate difference between the PE values statistically\n",
    "\n",
    "o\tPearsonâ€™s correlation coefficient to test the relationship between the PE values and pain levels?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bc34350aacd76694021ddea399eb98acc84d3e538da7095531cd73c20f546d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
