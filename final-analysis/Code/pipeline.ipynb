{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from numpy.fft import fft, ifft\n",
    "from scipy.signal import periodogram, welch\n",
    "import ordpy \n",
    "import statsmodels.api as sm\n",
    "from scipy.signal import butter,filtfilt\n",
    "from matplotlib.pyplot import figure\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from statsmodels.tsa.stattools import adfuller, pacf\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from scipy.stats import entropy\n",
    "from scipy import interpolate\n",
    "import ptitprince as pt\n",
    "import seaborn as sns\n",
    "import pingouin\n",
    "import nolds\n",
    "import EntropyHub as EH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.ceil(n * multiplier) / multiplier\n",
    "def round_down(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.floor(n * multiplier) / multiplier\n",
    "\n",
    "def flatten(seq):\n",
    "    l = []\n",
    "    for elt in seq:\n",
    "        t = type(elt)\n",
    "        if t is tuple or t is list:\n",
    "            for elt2 in flatten(elt):\n",
    "                l.append(elt2)\n",
    "        else:\n",
    "            l.append(elt)\n",
    "    return l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean, variance\n",
    "def mean(rating):\n",
    "    mean_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            mean_list.append(np.mean(rating[i]))\n",
    "        else:\n",
    "            mean_list.append(np.NaN)\n",
    "    return mean_list\n",
    "\n",
    "\n",
    "def variance(rating,time):\n",
    "    variance_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            variance = np.var(rating[i])\n",
    "            variance_list.append(variance)\n",
    "        else:\n",
    "            variance_list.append(np.NaN)\n",
    "    return variance_list\n",
    "\n",
    "def var_all(rating):\n",
    "    '''Calculate the variance of the concacenated time series'''\n",
    "    if len(rating) == 1:\n",
    "        rating_collected = rating[0]\n",
    "    elif len(rating) == 2:\n",
    "        rating_collected = np.hstack([rating[0], rating[1]])\n",
    "    elif len(rating) == 3:\n",
    "        rating_collected = np.hstack([rating[0], rating[1], rating[2]])\n",
    "    elif len(rating) == 4:\n",
    "        rating_collected = np.hstack([rating[0], rating[1], rating[2], rating[3]])\n",
    "    elif len(rating) == 5:\n",
    "        rating_collected = np.hstack([rating[0], rating[1], rating[2], rating[3], rating[4]])\n",
    "    else: \n",
    "        rating_collected = np.nan\n",
    "    return np.var(rating_collected)\n",
    "\n",
    "def cv(rating):\n",
    "    cv_list = []\n",
    "    for i in range(len(rating)):\n",
    "        cv_list.append(np.std(rating[i])/np.mean(rating[i]))\n",
    "    return cv_list\n",
    "def cv_all(rating):\n",
    "    '''Calculate the CV of the whole concacenated time series'''\n",
    "    if len(rating) == 1:\n",
    "        rating_collected = rating[0]\n",
    "    elif len(rating) == 2:\n",
    "        rating_collected = np.hstack([rating[0], rating[1]])\n",
    "    elif len(rating) == 3:\n",
    "        rating_collected = np.hstack([rating[0], rating[1], rating[2]])\n",
    "    elif len(rating) == 4:\n",
    "        rating_collected = np.hstack([rating[0], rating[1], rating[2], rating[3]])\n",
    "    elif len(rating) == 5:\n",
    "        rating_collected = np.hstack([rating[0], rating[1], rating[2], rating[3], rating[4]])\n",
    "    else: \n",
    "        rating_collected = np.nan\n",
    "    return np.std(rating_collected)/np.mean(rating_collected)\n",
    "\n",
    "#power-spectrum - welch method\n",
    "def power_spectrum(rating,time):\n",
    "    psd_freq_list = []\n",
    "    psd_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            N = len(rating[i])\n",
    "            T = time[i][-1]\n",
    "            f, S = welch(rating[i], 20)\n",
    "            psd_freq_list.append(f)\n",
    "            psd_list.append(S)\n",
    "        else:\n",
    "            psd_freq_list.append([np.NaN])\n",
    "            psd_list.append([np.NaN])\n",
    "    return psd_freq_list, psd_list\n",
    "\n",
    "\n",
    "#permutation entropy - complexity\n",
    "def permutation_entropy(rating):\n",
    "    entropy_list = []\n",
    "    stat_complexity_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            entropy, stat_complexity = ordpy.complexity_entropy(rating[i])\n",
    "            entropy_list.append(entropy)\n",
    "            stat_complexity_list.append(stat_complexity)\n",
    "        else:\n",
    "            entropy_list.append(np.NaN)\n",
    "            stat_complexity_list.append(np.NaN)\n",
    "    return entropy_list, stat_complexity_list\n",
    "\n",
    "#sample entropy\n",
    "def sample_entropy_func_default(rating):\n",
    "    entropy_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            entropy, A, B = EH.SampEn(rating[i])\n",
    "            entropy_list.append(entropy)\n",
    "        else:\n",
    "            entropy_list.append(np.NaN)\n",
    "    return entropy_list\n",
    "\n",
    "#autocorrelation\n",
    "def autocorrelation(rating, time):\n",
    "    acorr_list = []\n",
    "    time_lag_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            acorr = sm.tsa.acf(rating[i], nlags = int(len(rating[i])))\n",
    "            acorr_list.append(acorr)\n",
    "        else:\n",
    "            acorr_list.append([np.NaN, np.NaN, np.NaN])\n",
    "    for i in range(len(time)):\n",
    "        if len(time[i]) > 1:\n",
    "            time_lag = np.array(time[i]) - np.array(time[i][0])\n",
    "            time_lag_list.append(time_lag)\n",
    "        else:\n",
    "            time_lag_list.append([np.NaN, np.NaN, np.NaN])\n",
    "    return acorr_list, time_lag_list\n",
    "\n",
    "#check stationarity\n",
    "def adfuller_test(rating):\n",
    "    adfuller_list = []\n",
    "    stationary_list = []\n",
    "    for i in range(len(rating)):\n",
    "        if len(rating[i]) > 1:\n",
    "            result = adfuller(rating[i])\n",
    "            adfuller_list.append(result)\n",
    "            if result[1] <= 0.05:\n",
    "                stationary_list.append(1)\n",
    "            else:\n",
    "                stationary_list.append(0)\n",
    "        else:\n",
    "            adfuller_list.append(np.nan)\n",
    "            stationary_list.append(np.nan)\n",
    "    labels = ['ADF Test Statistic','p-value','#Lags Used','#Observation Used']\n",
    "    return adfuller_list, labels, stationary_list\n",
    "    \n",
    "# Hurst Exponent with DFA method\n",
    "def hurst_dfa(rating):\n",
    "    '''Hurst exponent calculated wwith DFA method. Fractal Dimension is calculated as D = 2-H where H is the Hurst exponent'''\n",
    "    poly_order = 1\n",
    "    rating = np.random.randn(10000)#rating_all[0][0]\n",
    "    window_sizes = [len(rating)//2,len(rating)//4,len(rating)//8,len(rating)//16,len(rating)//32]\n",
    "    F_n_all = []\n",
    "    for w in window_sizes:\n",
    "        X_t = np.cumsum(np.array(rating) - np.mean(rating))\n",
    "        segments = np.array_split(X_t, len(X_t)//w)\n",
    "        poly_coeffs_all = []\n",
    "        Y_t = []\n",
    "        for i in range(len(segments)):\n",
    "            poly_coeff = np.polyfit(np.arange(w), segments[i][:w], poly_order)\n",
    "            poly_coeffs_all.append(poly_coeff) #slope then intercept\n",
    "            Y_t_seg = poly_coeff[0]*np.arange(w)+poly_coeff[1]\n",
    "            for j in range(len(Y_t_seg)):\n",
    "                Y_t.append(Y_t_seg[j])\n",
    "        F_n = np.sqrt(np.sum((X_t[:len(Y_t)] - np.array(Y_t))**2)/len(X_t))\n",
    "        F_n_all.append(F_n)\n",
    "    #plt.scatter(np.log(window_sizes),np.log(F_n_all))\n",
    "    return np.polyfit(np.log(window_sizes),np.log(F_n_all),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../Data/data_file/D1_puregooddata_split_31032023.pkl')\n",
    "rating_all = []\n",
    "for i in range(0,len(data)):\n",
    "    rating_all.append(data['rating'].iloc[i])\n",
    "timing_all = []\n",
    "for i in range(0,len(data)):\n",
    "    timing_all.append(data['time'].iloc[i])\n",
    "participant_list = data['PID'].unique()\n",
    "rating_downsample_list_all = rating_all\n",
    "time_downsample_list_all = timing_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculations\n",
    "## Mean, Variance, CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list_all = []\n",
    "mean_square_list_all = []\n",
    "rms_list_all = []\n",
    "variance_list_all = []\n",
    "std_list_all = []\n",
    "\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "    mean_list = mean(rating_downsample_list_all[j])\n",
    "    variance_list= variance(rating_downsample_list_all[j], time_downsample_list_all[j])\n",
    "    mean_list_all.append(mean_list)\n",
    "    variance_list_all.append(variance_list)\n",
    "mean_overall = []\n",
    "for i in range(len(mean_list_all)):\n",
    "    mean_overall.append(np.mean(mean_list_all[i]))\n",
    "var_combined_list_all = []\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "    var_combined_list = var_all(rating_downsample_list_all[j])\n",
    "    var_combined_list_all.append(var_combined_list)\n",
    "\n",
    "cv_list_all = []\n",
    "\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "        cv_list = cv(rating_downsample_list_all[j])\n",
    "        cv_list_all.append(cv_list)\n",
    "        \n",
    "cv_combined_list = []\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "    cv_list = cv_all(rating_downsample_list_all[j])\n",
    "    cv_combined_list.append(cv_list)       \n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "psd_freq_list_all = []\n",
    "psd_list_all=[]\n",
    "\n",
    "\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "    psd_freq_list, psd_list = power_spectrum(rating_downsample_list_all[j], time_downsample_list_all[j])\n",
    "    psd_freq_list_all.append(psd_freq_list)\n",
    "    psd_list_all.append(psd_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "### Permutation Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_list_all = []\n",
    "stat_complexity_list_all = []\n",
    "for i in range(len(rating_downsample_list_all)):\n",
    "    #plt.figure()\n",
    "    entropy_list, stat_complexity_list = permutation_entropy(rating_downsample_list_all[i])\n",
    "    entropy_list_all.append(entropy_list)\n",
    "    stat_complexity_list_all.append(stat_complexity_list)\n",
    "entropy_overall = []\n",
    "for i in range(len(entropy_list_all)):\n",
    "    entropy_overall.append(np.mean(entropy_list_all[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_entropy_list_all = []\n",
    "for i in tqdm(range(len(rating_downsample_list_all))):\n",
    "    sample_entropy_list = sample_entropy_func_default(rating_downsample_list_all[i])\n",
    "    sample_entropy_list_all.append(sample_entropy_list)\n",
    "sample_entropy_overall = []\n",
    "for i in range(len(sample_entropy_list_all)):\n",
    "    sample_entropy_overall.append(np.mean(sample_entropy_list_all[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller_list_all = []\n",
    "stationary_list_all = []\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "    adfuller_list, labels, stationary_list = adfuller_test(rating_downsample_list_all[j])\n",
    "    adfuller_list_all.append(adfuller_list)\n",
    "    stationary_list_all.append(stationary_list)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acorr_list_all = []\n",
    "time_lag_list_all = []\n",
    "for j in range(len(rating_downsample_list_all)):\n",
    "        \n",
    "    #plt.figure()\n",
    "    acorr_list, time_lag = autocorrelation(rating_downsample_list_all[j], time_downsample_list_all[j])\n",
    "    acorr_list_all.append(acorr_list)\n",
    "    time_lag_list_all.append(time_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_time_scale_index = []\n",
    "char_time_scale = []\n",
    "for i in range(len(acorr_list_all)):\n",
    "    char_time_scale_index_part = []\n",
    "    char_time_scale_part = []\n",
    "    for j in range(len(acorr_list_all[i])):\n",
    "        time_index = np.argmax(acorr_list_all[i][j] < 0.1)\n",
    "        char_time_scale_index_part.append(time_index)\n",
    "        char_time_scale_part.append(time_lag_list_all[i][j][time_index])\n",
    "\n",
    "    char_time_scale_index.append(char_time_scale_index_part)\n",
    "    char_time_scale.append(char_time_scale_part)\n",
    "char_time_scale_mean = []\n",
    "for i in range(len(char_time_scale)):\n",
    "    char_time_scale_mean.append(np.mean(char_time_scale[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fractal Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hurst_dfa_all = []\n",
    "fractal_dfa_all = []\n",
    "for i in range(len(rating_downsample_list_all)):\n",
    "    hurst_dfa_list = []\n",
    "    fractal_list = []\n",
    "    for j in range(len(rating_downsample_list_all[i])):\n",
    "        hurst_dllm = hurst_dfa(rating_downsample_list_all[i][j])\n",
    "        hurst_dfa_list.append(hurst_dllm[0])\n",
    "        fractal_list.append(2-hurst_dllm[0])\n",
    "    hurst_dfa_all.append(hurst_dfa_list)\n",
    "    fractal_dfa_all.append(fractal_list)\n",
    "\n",
    "\n",
    "fractal_dfa_overall = []\n",
    "for i in range(len(fractal_dfa_all)):\n",
    "    fractal_dfa_overall.append(np.nanmean(fractal_dfa_all[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.arima import auto_arima\n",
    "def calcsmape(actual, forecast):\n",
    "    return 1/len(actual) * np.sum(2 * np.abs(forecast-actual) / (np.abs(actual) + np.abs(forecast)))\n",
    "\n",
    "length_data = []\n",
    "for i in range(len(rating_downsample_list_all)):\n",
    "    for j in range(len(rating_downsample_list_all[i])):\n",
    "        length_data.append(len(rating_downsample_list_all[i][j]))\n",
    "\n",
    "\n",
    "TEST_SIZE = 100\n",
    "smape_all = []\n",
    "model_all = []\n",
    "for i in tqdm(range(len(rating_downsample_list_all))):\n",
    "    smape_participant = []\n",
    "    model_participant = []\n",
    "    for j in range(len(rating_downsample_list_all[i])):\n",
    "        data_trial = pd.Series(rating_downsample_list_all[i][j])\n",
    "        train, test = data_trial[:-TEST_SIZE], data_trial[-TEST_SIZE:]\n",
    "\n",
    "        x_train, x_test = np.array(range(train.shape[0])), np.array(range(train.shape[0], data_trial.shape[0]))\n",
    "        train.shape, x_train.shape, test.shape, x_test.shape\n",
    "        model = auto_arima(train, start_p=1, start_q=1,\n",
    "                            test='adf',\n",
    "                            max_p=5, max_q=5,\n",
    "                            m=1,             \n",
    "                            d=1,          \n",
    "                            seasonal=False,   \n",
    "                            start_P=0, \n",
    "                            D=None, \n",
    "                            trace=True,\n",
    "                            error_action='ignore',  \n",
    "                            suppress_warnings=True, \n",
    "                            stepwise=True)\n",
    "        model_participant.append(model)\n",
    "        prediction, confint = model.predict(n_periods=TEST_SIZE, return_conf_int=True)\n",
    "        cf= pd.DataFrame(confint)\n",
    "        prediction_series = pd.Series(prediction, index = test.index)\n",
    "        fig1, ax1 = plt.subplots(1, 1, figsize=(15, 5))\n",
    "        ax1.plot(data_trial, label = 'original')\n",
    "        ax1.plot(prediction_series, label = 'forecasting')\n",
    "        ax1.fill_between(prediction_series.index,\n",
    "                        cf[0],\n",
    "                        cf[1],color='grey',alpha=.3)\n",
    "        ax1.set_ylim(0,10)\n",
    "        ax1.set_title(f'participant id {participant_list[i]}, day 1, trial {j+1}')\n",
    "        ax1.legend()\n",
    "        smape=calcsmape(test,prediction)\n",
    "        smape_participant.append(smape)\n",
    "    smape_all.append(smape_participant)\n",
    "    model_all.append(model_participant)\n",
    "smape_mean_all = []\n",
    "for i in range(len(smape_all)):\n",
    "    smape_mean_all.append(np.mean(smape_all[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all the data files and its path\n",
    "questionnaire = []\n",
    "onlyfiles = [f for f in listdir('../Data/questionnaire') if isfile(join('../Data/questionnaire', f))]\n",
    "for i in range(len(onlyfiles)):\n",
    "    path = '../Data/questionnaire/' + onlyfiles[i]\n",
    "    questionnaire.append(path)\n",
    "\n",
    "\n",
    "df_questionnaire_pre = pd.read_csv(questionnaire[0])\n",
    "df_questionnaire_s1 = pd.read_csv(questionnaire[1])\n",
    "df_questionnaire_s2 = pd.read_csv(questionnaire[2])\n",
    "index_pre = []\n",
    "for id in participant_list:\n",
    "    index_pre.append(np.where(df_questionnaire_pre.PROLIFIC_PID == id)[0][0])\n",
    "index_s1 = []\n",
    "for id in participant_list:\n",
    "    index_s1.append(np.where(df_questionnaire_s1.PROLIFIC_PID == id)[0])\n",
    "index_s2 = []\n",
    "for id in participant_list:\n",
    "    index_s2.append(np.where(df_questionnaire_s2.PROLIFIC_PID == id)[0])\n",
    "df_questionnaire_index_pre = df_questionnaire_pre.iloc[index_pre]\n",
    "msk_scores = df_questionnaire_index_pre['Score'].values\n",
    "msk_last_q_corr = np.load('msk_last_q.npy')\n",
    "index_s1_normal = []\n",
    "for i in range(len(index_s1)):\n",
    "    if len(index_s1[i]) == 1:\n",
    "        index_s1_normal.append(index_s1[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s1_normal = df_questionnaire_s1.iloc[index_s1_normal]\n",
    "participant_list_s1_questionnaire = df_s1_normal.PROLIFIC_PID.values\n",
    "s1_questionnaire_index = []\n",
    "for i in range(len(participant_list)):\n",
    "    if participant_list[i] in participant_list_s1_questionnaire:\n",
    "        s1_questionnaire_index.append(i)\n",
    "mean_s1 = np.array(mean_overall)[s1_questionnaire_index]\n",
    "var_s1 = np.array(var_combined_list_all)[s1_questionnaire_index]\n",
    "entropy_s1 = np.array(entropy_overall)[s1_questionnaire_index]\n",
    "cv_s1 = np.array(cv_combined_list)[s1_questionnaire_index]\n",
    "acf_s1 = np.array(char_time_scale_mean)[s1_questionnaire_index]\n",
    "smape_s1 = np.array(smape_mean_all)[s1_questionnaire_index]\n",
    "sample_entropy_s1 = np.array(sample_entropy_overall)[s1_questionnaire_index]\n",
    "index_s2_normal = []\n",
    "for i in range(len(index_s2)):\n",
    "    if len(index_s2[i]) == 1:\n",
    "        index_s2_normal.append(index_s2[i][0])\n",
    "df_s2_normal = df_questionnaire_s2.iloc[index_s2_normal]\n",
    "participant_list_s2_questionnaire = df_s2_normal.PROLIFIC_PID.values#.to_list()\n",
    "s2_questionnaire_index = []\n",
    "for i in range(len(participant_list)):\n",
    "    if participant_list[i] in participant_list_s2_questionnaire:\n",
    "        s2_questionnaire_index.append(i)\n",
    "mean_s2 = np.array(mean_overall)[s2_questionnaire_index]\n",
    "var_s2 = np.array(var_combined_list_all)[s2_questionnaire_index]\n",
    "entropy_s2 = np.array(entropy_overall)[s2_questionnaire_index]\n",
    "cv_s2 = np.array(cv_combined_list)[s2_questionnaire_index]\n",
    "acf_s2 = np.array(char_time_scale_mean)[s2_questionnaire_index]\n",
    "smape_s2 = np.array(smape_mean_all)[s2_questionnaire_index]\n",
    "sample_entropy_s2 = np.array(sample_entropy_overall)[s2_questionnaire_index]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code to find correlation coefficient and BF10 - just change the name of the questionnaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-screening (msk_bf, msk_lastq_bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean')\n",
    "mean_mskscores = zip(mean_overall, msk_scores)\n",
    "x = []; y=[]\n",
    "for point in list(mean_mskscores):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "x_bestfit = np.arange(0,11,0.1)\n",
    "y_bestfit = a*x_bestfit+b\n",
    "corr_mean_mskscores = np.corrcoef(x,y)\n",
    "print(corr_mean_mskscores)\n",
    "msk_bf = []\n",
    "bf_mean_msk = pingouin.bayesfactor_pearson(corr_mean_mskscores[0][1], len(x))\n",
    "msk_bf.append(bf_mean_msk)\n",
    "print(bf_mean_msk)\n",
    "\n",
    "print('---------')\n",
    "print('variance')\n",
    "var_mskscores = zip(var_combined_list_all, msk_scores)\n",
    "x = []; y=[]\n",
    "for point in list(var_mskscores):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_var_mskscores = np.corrcoef(x,y)\n",
    "print(corr_var_mskscores)\n",
    "bf_var_msk = pingouin.bayesfactor_pearson(corr_var_mskscores[0][1], len(x))\n",
    "msk_bf.append(bf_var_msk)\n",
    "print(bf_var_msk)\n",
    "\n",
    "\n",
    "print('---------')\n",
    "print('CV')\n",
    "fano_mskscores = zip(cv_combined_list, msk_scores)\n",
    "x = []; y=[]\n",
    "for point in list(fano_mskscores):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_fano_mskscores = np.corrcoef(x,y)\n",
    "print(corr_fano_mskscores)\n",
    "bf_fano_msk = pingouin.bayesfactor_pearson(corr_fano_mskscores[0][1], len(x))\n",
    "msk_bf.append(bf_fano_msk)\n",
    "print(bf_fano_msk)\n",
    "\n",
    "print('---------')\n",
    "print('PE')\n",
    "pe_mskscores = zip(entropy_overall, msk_scores)\n",
    "x = []; y=[]\n",
    "for point in list(pe_mskscores):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_pe_mskscores = np.corrcoef(x,y)\n",
    "print(corr_pe_mskscores)\n",
    "bf_entropy_msk = pingouin.bayesfactor_pearson(corr_pe_mskscores[0][1], len(x))\n",
    "msk_bf.append(bf_entropy_msk)\n",
    "print(bf_entropy_msk)\n",
    "\n",
    "print('---------')\n",
    "print('SampEn')\n",
    "sampen_mskscores = zip(sample_entropy_overall, msk_scores)\n",
    "x = []; y=[]\n",
    "for point in list(sampen_mskscores):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_sampen_mskscores = np.corrcoef(x,y)\n",
    "print(corr_sampen_mskscores)\n",
    "bf_sampen_msk = pingouin.bayesfactor_pearson(corr_sampen_mskscores[0][1], len(x))\n",
    "msk_bf.append(bf_sampen_msk)\n",
    "print(bf_sampen_msk)\n",
    "\n",
    "print('---------')\n",
    "print('acf')\n",
    "acf_mskscores = zip(char_time_scale_mean, msk_scores)\n",
    "x = []; y=[]\n",
    "for point in list(acf_mskscores):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_acf_mskscores = np.corrcoef(x,y)\n",
    "print(corr_acf_mskscores)\n",
    "bf_acf_msk = pingouin.bayesfactor_pearson(corr_acf_mskscores[0][1], len(x))\n",
    "msk_bf.append(bf_acf_msk)\n",
    "print(bf_acf_msk)\n",
    "\n",
    "print('---------')\n",
    "print('smape')\n",
    "smape_mskscores = zip(smape_mean_all, msk_scores)\n",
    "x = []; y=[]\n",
    "for point in list(smape_mskscores):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_smape_mskscores = np.corrcoef(x,y)\n",
    "print(corr_smape_mskscores)\n",
    "bf_smape_msk = pingouin.bayesfactor_pearson(corr_smape_mskscores[0][1], len(x))\n",
    "msk_bf.append(bf_smape_msk)\n",
    "print(bf_smape_msk)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('mean')\n",
    "mean_mskscores_lastq = zip(mean_overall, msk_last_q_corr)\n",
    "x = []; y=[]\n",
    "for point in list(mean_mskscores_lastq):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "x_bestfit = np.arange(0,11,0.1)\n",
    "y_bestfit = a*x_bestfit+b\n",
    "corr_mean_mskscores_lastq = np.corrcoef(x,y)\n",
    "print(corr_mean_mskscores_lastq)\n",
    "msk_lastq_bf = []\n",
    "bf_mean_msk_lastq = pingouin.bayesfactor_pearson(corr_mean_mskscores_lastq[0][1], len(x))\n",
    "msk_lastq_bf.append(bf_mean_msk_lastq)\n",
    "print(bf_mean_msk_lastq)\n",
    "\n",
    "print('---------')\n",
    "print('variance')\n",
    "var_mskscores_lastq = zip(var_combined_list_all, msk_last_q_corr)\n",
    "x = []; y=[]\n",
    "for point in list(var_mskscores_lastq):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_var_mskscores_lastq = np.corrcoef(x,y)\n",
    "print(corr_var_mskscores_lastq)\n",
    "bf_var_msk_lastq = pingouin.bayesfactor_pearson(corr_var_mskscores_lastq[0][1], len(x))\n",
    "msk_lastq_bf.append(bf_var_msk_lastq)\n",
    "print(bf_var_msk_lastq)\n",
    "\n",
    "\n",
    "print('---------')\n",
    "print('CV')\n",
    "fano_mskscores_lastq = zip(cv_combined_list, msk_last_q_corr)\n",
    "x = []; y=[]\n",
    "for point in list(fano_mskscores_lastq):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_fano_mskscores_lastq = np.corrcoef(x,y)\n",
    "print(corr_fano_mskscores_lastq)\n",
    "bf_fano_msk_lastq = pingouin.bayesfactor_pearson(corr_fano_mskscores_lastq[0][1], len(x))\n",
    "msk_lastq_bf.append(bf_fano_msk_lastq)\n",
    "print(bf_fano_msk_lastq)\n",
    "\n",
    "print('---------')\n",
    "print('PE')\n",
    "pe_mskscores_lastq = zip(entropy_overall, msk_last_q_corr)\n",
    "x = []; y=[]\n",
    "for point in list(pe_mskscores_lastq):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_pe_mskscores_lastq = np.corrcoef(x,y)\n",
    "print(corr_pe_mskscores_lastq)\n",
    "bf_entropy_msk_lastq = pingouin.bayesfactor_pearson(corr_pe_mskscores_lastq[0][1], len(x))\n",
    "msk_lastq_bf.append(bf_entropy_msk_lastq)\n",
    "print(bf_entropy_msk_lastq)\n",
    "\n",
    "print('---------')\n",
    "print('SampEn')\n",
    "sampen_mskscores_lastq = zip(sample_entropy_overall, msk_last_q_corr)\n",
    "x = []; y=[]\n",
    "for point in list(sampen_mskscores_lastq):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_sampen_mskscores_lastq = np.corrcoef(x,y)\n",
    "print(corr_sampen_mskscores_lastq)\n",
    "bf_sampen_msk_lastq = pingouin.bayesfactor_pearson(corr_sampen_mskscores[0][1], len(x))\n",
    "msk_lastq_bf.append(bf_sampen_msk_lastq)\n",
    "print(bf_sampen_msk_lastq)\n",
    "\n",
    "print('---------')\n",
    "print('acf')\n",
    "acf_mskscores_lastq = zip(char_time_scale_mean, msk_last_q_corr)\n",
    "x = []; y=[]\n",
    "for point in list(acf_mskscores_lastq):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_acf_mskscores_lastq = np.corrcoef(x,y)\n",
    "print(corr_acf_mskscores_lastq)\n",
    "bf_acf_msk_lastq = pingouin.bayesfactor_pearson(corr_acf_mskscores_lastq[0][1], len(x))\n",
    "msk_lastq_bf.append(bf_acf_msk_lastq)\n",
    "print(bf_acf_msk_lastq)\n",
    "\n",
    "print('---------')\n",
    "print('smape')\n",
    "smape_mskscores_lastq = zip(smape_mean_all, msk_last_q_corr)\n",
    "x = []; y=[]\n",
    "for point in list(smape_mskscores_lastq):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_smape_mskscores_lastq = np.corrcoef(x,y)\n",
    "print(corr_smape_mskscores_lastq)\n",
    "bf_smape_msk_lastq = pingouin.bayesfactor_pearson(corr_smape_mskscores_lastq[0][1], len(x))\n",
    "msk_lastq_bf.append(bf_smape_msk_lastq)\n",
    "print(bf_smape_msk_lastq)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## section 1 (bpisev_bf, bpiint_bf, start_bf, startsub_bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean')\n",
    "bpi_mean_severity = df_s1_normal['BPI Mean Severity Score'].values\n",
    "mean_bpisev = zip(mean_s1, bpi_mean_severity)\n",
    "x = []; y=[]\n",
    "for point in list(mean_bpisev):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_mean_bpisev = np.corrcoef(x,y)\n",
    "print(corr_mean_bpisev)\n",
    "bpisev_bf = []\n",
    "bf_mean_bpisev = pingouin.bayesfactor_pearson(corr_mean_bpisev[0][1], len(x))\n",
    "bpisev_bf.append(bf_mean_bpisev)\n",
    "print(bf_mean_bpisev)\n",
    "print('------')\n",
    "print('variance')\n",
    "var_bpisev = zip(var_s1, bpi_mean_severity)\n",
    "x = []; y=[]\n",
    "for point in list(var_bpisev):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_var_bpisev = np.corrcoef(x,y)\n",
    "print(corr_var_bpisev)\n",
    "bf_var_bpisev = pingouin.bayesfactor_pearson(corr_var_bpisev[0][1], len(x))\n",
    "bpisev_bf.append(bf_var_bpisev)\n",
    "print(bf_var_bpisev)\n",
    "\n",
    "print('------')\n",
    "print('CV')\n",
    "fano_bpisev = zip(cv_s1, bpi_mean_severity)\n",
    "x = []; y=[]\n",
    "for point in list(fano_bpisev):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_fano_bpisev = np.corrcoef(x,y)\n",
    "print(corr_fano_bpisev)\n",
    "bf_fano_bpisev = pingouin.bayesfactor_pearson(corr_fano_bpisev[0][1], len(x))\n",
    "bpisev_bf.append(bf_fano_bpisev)\n",
    "print(bf_fano_bpisev)\n",
    "\n",
    "print('------')\n",
    "print('PE')\n",
    "entropy_bpisev = zip(entropy_s1, bpi_mean_severity)\n",
    "x = []; y=[]\n",
    "for point in list(entropy_bpisev):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_entropy_bpisev = np.corrcoef(x,y)\n",
    "print(corr_entropy_bpisev)\n",
    "bf_entropy_bpisev = pingouin.bayesfactor_pearson(corr_entropy_bpisev[0][1], len(x))\n",
    "bpisev_bf.append(bf_entropy_bpisev)\n",
    "print(bf_entropy_bpisev)\n",
    "\n",
    "print('------')\n",
    "print('SampEn')\n",
    "sample_entropy_bpisev = zip(sample_entropy_s1, bpi_mean_severity)\n",
    "x = []; y=[]\n",
    "for point in list(sample_entropy_bpisev):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_sample_entropy_bpisev = np.corrcoef(x,y)\n",
    "print(corr_sample_entropy_bpisev)\n",
    "bf_sample_entropy_bpisev = pingouin.bayesfactor_pearson(corr_sample_entropy_bpisev[0][1], len(x))\n",
    "bpisev_bf.append(bf_sample_entropy_bpisev)\n",
    "print(bf_sample_entropy_bpisev)\n",
    "\n",
    "print('------')\n",
    "print('ACF')\n",
    "acf_bpisev = zip(acf_s1, bpi_mean_severity)\n",
    "x = []; y=[]\n",
    "for point in list(acf_bpisev):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_acf_bpisev = np.corrcoef(x,y)\n",
    "print(corr_acf_bpisev)\n",
    "bf_acf_bpisev = pingouin.bayesfactor_pearson(corr_acf_bpisev[0][1], len(x))\n",
    "bpisev_bf.append(bf_acf_bpisev)\n",
    "print(bf_acf_bpisev)\n",
    "\n",
    "print('------')\n",
    "print('smape')\n",
    "smape_bpisev = zip(smape_s1, bpi_mean_severity)\n",
    "x = []; y=[]\n",
    "for point in list(smape_bpisev):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_smape_bpisev = np.corrcoef(x,y)\n",
    "print(corr_smape_bpisev)\n",
    "bf_smape_bpisev = pingouin.bayesfactor_pearson(corr_smape_bpisev[0][1], len(x))\n",
    "bpisev_bf.append(bf_smape_bpisev)\n",
    "print(bf_smape_bpisev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpi_mean_severity = df_s1_normal['BPI Mean Severity Score'].values\n",
    "fano_bpisev = zip(cv_s1, bpi_mean_severity)\n",
    "x = []; y=[]\n",
    "for point in list(fano_bpisev):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_fano_bpisev = np.corrcoef(x,y)\n",
    "print(corr_fano_bpisev)\n",
    "bf_fano_bpisev = pingouin.bayesfactor_pearson(corr_fano_bpisev[0][1], len(x))\n",
    "print(bf_fano_bpisev)\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "x_bestfit = np.arange(0,2,0.1)\n",
    "y_bestfit = a*x_bestfit+b\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y)\n",
    "ax.set_xlabel('CV')\n",
    "ax.set_ylabel('score')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_title('BPI mean severity score against CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean')\n",
    "bpi_mean_interference = df_s1_normal['BPI Mean Pain Interference Score'].values\n",
    "mean_bpiint = zip(mean_s1, bpi_mean_interference)\n",
    "x = []; y=[]\n",
    "for point in list(mean_bpiint):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_mean_bpiint = np.corrcoef(x,y)\n",
    "print(corr_mean_bpiint)\n",
    "bpiint_bf = []\n",
    "bf_mean_bpiint = pingouin.bayesfactor_pearson(corr_mean_bpiint[0][1], len(x))\n",
    "bpiint_bf.append(bf_mean_bpiint)\n",
    "print(bf_mean_bpiint)\n",
    "\n",
    "print('------')\n",
    "print('variance')\n",
    "var_bpiint = zip(var_s1, bpi_mean_interference)\n",
    "x = []; y=[]\n",
    "for point in list(var_bpiint):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_var_bpiint = np.corrcoef(x,y)\n",
    "print(corr_var_bpiint)\n",
    "bf_var_bpiint = pingouin.bayesfactor_pearson(corr_var_bpiint[0][1], len(x))\n",
    "bpiint_bf.append(bf_var_bpiint)\n",
    "print(bf_var_bpiint)\n",
    "\n",
    "\n",
    "print('------')\n",
    "print('CV')\n",
    "fano_bpiint = zip(cv_s1, bpi_mean_interference)\n",
    "x = []; y=[]\n",
    "for point in list(fano_bpiint):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_fano_bpiint = np.corrcoef(x,y)\n",
    "print(corr_fano_bpiint)\n",
    "bf_fano_bpiint = pingouin.bayesfactor_pearson(corr_fano_bpiint[0][1], len(x))\n",
    "bpiint_bf.append(bf_fano_bpiint)\n",
    "print(bf_fano_bpiint)\n",
    "\n",
    "print('------')\n",
    "print('PE')\n",
    "entropy_bpiint = zip(entropy_s1, bpi_mean_interference)\n",
    "x = []; y=[]\n",
    "for point in list(entropy_bpiint):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_entropy_bpiint = np.corrcoef(x,y)\n",
    "print(corr_entropy_bpiint)\n",
    "bf_entropy_bpiint = pingouin.bayesfactor_pearson(corr_entropy_bpiint[0][1], len(x))\n",
    "bpiint_bf.append(bf_entropy_bpiint)\n",
    "print(bf_entropy_bpiint)\n",
    "\n",
    "print('------')\n",
    "print('SampEn')\n",
    "sample_entropy_bpiint = zip(sample_entropy_s1, bpi_mean_interference)\n",
    "x = []; y=[]\n",
    "for point in list(sample_entropy_bpiint):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_sample_entropy_bpiint = np.corrcoef(x,y)\n",
    "print(corr_sample_entropy_bpiint)\n",
    "bf_sample_entropy_bpiint = pingouin.bayesfactor_pearson(corr_sample_entropy_bpiint[0][1], len(x))\n",
    "bpiint_bf.append(bf_sample_entropy_bpiint)\n",
    "print(bf_sample_entropy_bpiint)\n",
    "\n",
    "print('------')\n",
    "print('acf')\n",
    "acf_bpiint = zip(acf_s1, bpi_mean_interference)\n",
    "x = []; y=[]\n",
    "for point in list(acf_bpiint):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_acf_bpiint = np.corrcoef(x,y)\n",
    "print(corr_acf_bpiint)\n",
    "bf_acf_bpiint = pingouin.bayesfactor_pearson(corr_acf_bpiint[0][1], len(x))\n",
    "bpiint_bf.append(bf_acf_bpiint)\n",
    "print(bf_acf_bpiint)\n",
    "\n",
    "print('------')\n",
    "print('smape')\n",
    "smape_bpiint = zip(smape_s1, bpi_mean_interference)\n",
    "x = []; y=[]\n",
    "for point in list(smape_bpiint):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_smape_bpiint = np.corrcoef(x,y)\n",
    "print(corr_smape_bpiint)\n",
    "bf_smape_bpiint = pingouin.bayesfactor_pearson(corr_smape_bpiint[0][1], len(x))\n",
    "bpiint_bf.append(bf_smape_bpiint)\n",
    "print(bf_smape_bpiint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_pain = df_s1_normal['Back Pain'].values == 'Yes'\n",
    "print('mean')\n",
    "start_total = df_s1_normal['START total score'].iloc[back_pain].values\n",
    "mean_start = zip(mean_s1[back_pain], start_total)\n",
    "x = []; y=[]\n",
    "for point in list(mean_start):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_mean_start = np.corrcoef(x,y)\n",
    "print(corr_mean_start)\n",
    "start_bf = []\n",
    "bf_mean_start = pingouin.bayesfactor_pearson(corr_mean_start[0][1], len(x))\n",
    "start_bf.append(bf_mean_start)\n",
    "print(bf_mean_start)\n",
    "print('-----')\n",
    "print('variance')\n",
    "var_start = zip(var_s1[back_pain], start_total)\n",
    "x = []; y=[]\n",
    "for point in list(var_start):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_var_start = np.corrcoef(x,y)\n",
    "print(corr_var_start)\n",
    "bf_var_start = pingouin.bayesfactor_pearson(corr_var_start[0][1], len(x))\n",
    "start_bf.append(bf_var_start)\n",
    "print(bf_var_start)\n",
    "\n",
    "print('-----')\n",
    "print('CV')\n",
    "fano_start = zip(cv_s1[back_pain], start_total)\n",
    "x = []; y=[]\n",
    "for point in list(fano_start):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_fano_start = np.corrcoef(x,y)\n",
    "print(corr_fano_start)\n",
    "bf_fano_start = pingouin.bayesfactor_pearson(corr_fano_start[0][1], len(x))\n",
    "start_bf.append(bf_fano_start)\n",
    "print(bf_fano_start)\n",
    "\n",
    "print('-----')\n",
    "print('PE')\n",
    "entropy_start = zip(entropy_s1[back_pain], start_total)\n",
    "x = []; y=[]\n",
    "for point in list(entropy_start):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_entropy_start = np.corrcoef(x,y)\n",
    "print(corr_entropy_start)\n",
    "bf_entropy_start = pingouin.bayesfactor_pearson(corr_entropy_start[0][1], len(x))\n",
    "start_bf.append(bf_entropy_start)\n",
    "print(bf_entropy_start)\n",
    "\n",
    "print('-----')\n",
    "print('SampEn')\n",
    "sample_entropy_start = zip(sample_entropy_s1[back_pain], start_total)\n",
    "x = []; y=[]\n",
    "for point in list(sample_entropy_start):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_sample_entropy_start = np.corrcoef(x,y)\n",
    "print(corr_sample_entropy_start)\n",
    "bf_sample_entropy_start = pingouin.bayesfactor_pearson(corr_sample_entropy_start[0][1], len(x))\n",
    "start_bf.append(bf_sample_entropy_start)\n",
    "print(bf_sample_entropy_start)\n",
    "\n",
    "print('-----')\n",
    "print('acf')\n",
    "acf_start = zip(acf_s1[back_pain], start_total)\n",
    "x = []; y=[]\n",
    "for point in list(acf_start):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_acf_start = np.corrcoef(x,y)\n",
    "print(corr_acf_start)\n",
    "bf_acf_start = pingouin.bayesfactor_pearson(corr_acf_start[0][1], len(x))\n",
    "start_bf.append(bf_acf_start)\n",
    "print(bf_acf_start)\n",
    "\n",
    "print('-----')\n",
    "print('smape')\n",
    "smape_start = zip(smape_s1[back_pain], start_total)\n",
    "x = []; y=[]\n",
    "for point in list(smape_start):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_smape_start = np.corrcoef(x,y)\n",
    "print(corr_smape_start)\n",
    "bf_smape_start = pingouin.bayesfactor_pearson(corr_smape_start[0][1], len(x))\n",
    "start_bf.append(bf_smape_start)\n",
    "print(bf_smape_start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sub = df_s1_normal['START sub score'].iloc[back_pain].values\n",
    "print('mean')\n",
    "mean_startsub = zip(mean_s1, start_sub)\n",
    "x = []; y=[]\n",
    "for point in list(mean_startsub):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_mean_startsub = np.corrcoef(x,y)\n",
    "print(corr_mean_startsub)\n",
    "startsub_bf = []\n",
    "bf_mean_startsub = pingouin.bayesfactor_pearson(corr_mean_startsub[0][1], len(x))\n",
    "startsub_bf.append(bf_mean_startsub)\n",
    "print(bf_mean_startsub)\n",
    "print('-----')\n",
    "print('variance')\n",
    "var_startsub = zip(var_s1, start_sub)\n",
    "x = []; y=[]\n",
    "for point in list(var_startsub):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_var_startsub = np.corrcoef(x,y)\n",
    "print(corr_var_startsub)\n",
    "bf_var_startsub = pingouin.bayesfactor_pearson(corr_var_startsub[0][1], len(x))\n",
    "startsub_bf.append(bf_var_startsub)\n",
    "print(bf_var_startsub)\n",
    "\n",
    "print('-----')\n",
    "print('CV')\n",
    "fano_startsub = zip(cv_s1, start_sub)\n",
    "x = []; y=[]\n",
    "for point in list(fano_startsub):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_fano_startsub = np.corrcoef(x,y)\n",
    "print(corr_fano_startsub)\n",
    "bf_fano_startsub = pingouin.bayesfactor_pearson(corr_fano_startsub[0][1], len(x))\n",
    "startsub_bf.append(bf_fano_startsub)\n",
    "print(bf_fano_startsub)\n",
    "\n",
    "print('-----')\n",
    "print('PE')\n",
    "entropy_startsub = zip(entropy_s1, start_sub)\n",
    "x = []; y=[]\n",
    "for point in list(entropy_startsub):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_entropy_startsub = np.corrcoef(x,y)\n",
    "print(corr_entropy_startsub)\n",
    "bf_entropy_startsub = pingouin.bayesfactor_pearson(corr_entropy_startsub[0][1], len(x))\n",
    "startsub_bf.append(bf_entropy_startsub)\n",
    "print(bf_entropy_startsub)\n",
    "\n",
    "print('-----')\n",
    "print('SampEn')\n",
    "sample_entropy_startsub = zip(sample_entropy_s1, start_sub)\n",
    "x = []; y=[]\n",
    "for point in list(sample_entropy_startsub):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_sample_entropy_startsub = np.corrcoef(x,y)\n",
    "print(corr_sample_entropy_startsub)\n",
    "bf_sample_entropy_startsub = pingouin.bayesfactor_pearson(corr_sample_entropy_startsub[0][1], len(x))\n",
    "startsub_bf.append(bf_sample_entropy_startsub)\n",
    "print(bf_sample_entropy_startsub)\n",
    "\n",
    "print('-----')\n",
    "print('acf')\n",
    "acf_startsub = zip(acf_s1, start_sub)\n",
    "x = []; y=[]\n",
    "for point in list(acf_startsub):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_acf_startsub = np.corrcoef(x,y)\n",
    "print(corr_acf_startsub)\n",
    "bf_acf_startsub = pingouin.bayesfactor_pearson(corr_acf_startsub[0][1], len(x))\n",
    "startsub_bf.append(bf_acf_startsub)\n",
    "print(bf_acf_startsub)\n",
    "print('-----')\n",
    "print('smape')\n",
    "smape_startsub = zip(smape_s1, start_sub)\n",
    "x = []; y=[]\n",
    "for point in list(smape_startsub):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_smape_startsub = np.corrcoef(x,y)\n",
    "print(corr_smape_startsub)\n",
    "bf_smape_startsub = pingouin.bayesfactor_pearson(corr_smape_startsub[0][1], len(x))\n",
    "startsub_bf.append(bf_smape_startsub)\n",
    "print(bf_smape_startsub)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## section 2 (pcs_total_bf, pcs_rum_bf, pcs_mag_bf, pcs_help_bf, gad7_bf, phq9_bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_s2_normal = []\n",
    "for i in range(len(index_s2)):\n",
    "    if len(index_s2[i]) == 1:\n",
    "        index_s2_normal.append(index_s2[i][0])\n",
    "df_s2_normal = df_questionnaire_s2.iloc[index_s2_normal]\n",
    "participant_list_s2_questionnaire = df_s2_normal.PROLIFIC_PID.values#.to_list()\n",
    "s2_questionnaire_index = []\n",
    "for i in range(len(participant_list)):\n",
    "    if participant_list[i] in participant_list_s2_questionnaire:\n",
    "        s2_questionnaire_index.append(i)\n",
    "mean_s2 = np.array(mean_overall)[s2_questionnaire_index]\n",
    "var_s2 = np.array(var_combined_list_all)[s2_questionnaire_index]\n",
    "entropy_s2 = np.array(entropy_overall)[s2_questionnaire_index]\n",
    "cv_s2 = np.array(cv_combined_list)[s2_questionnaire_index]\n",
    "acf_s2 = np.array(char_time_scale_mean)[s2_questionnaire_index]\n",
    "smape_s2 = np.array(smape_mean_all)[s2_questionnaire_index]\n",
    "sample_entropy_s2 = np.array(sample_entropy_overall)[s2_questionnaire_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_total = df_s2_normal['Total PCS Score'].values\n",
    "print('mean')\n",
    "mean_pcs_total = zip(mean_s2, pcs_total)\n",
    "x = []; y=[]\n",
    "for point in list(mean_pcs_total):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_mean_pcs_total = np.corrcoef(x,y)\n",
    "print(corr_mean_pcs_total)\n",
    "pcs_total_bf = []\n",
    "bf_mean_pcs_total = pingouin.bayesfactor_pearson(corr_mean_pcs_total[0][1], len(x))\n",
    "pcs_total_bf.append(bf_mean_pcs_total)\n",
    "print(bf_mean_pcs_total)\n",
    "print('-----')\n",
    "print('variance')\n",
    "var_pcs_total = zip(var_s2, pcs_total)\n",
    "x = []; y=[]\n",
    "for point in list(var_pcs_total):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_var_pcs_total = np.corrcoef(x,y)\n",
    "print(corr_var_pcs_total)\n",
    "bf_var_pcs_total = pingouin.bayesfactor_pearson(corr_var_pcs_total[0][1], len(x))\n",
    "pcs_total_bf.append(bf_var_pcs_total)\n",
    "print(bf_var_pcs_total)\n",
    "\n",
    "print('-----')\n",
    "print('CV')\n",
    "fano_pcs_total = zip(cv_s2, pcs_total)\n",
    "x = []; y=[]\n",
    "for point in list(fano_pcs_total):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "corr_fano_pcs_total = np.corrcoef(x,y)\n",
    "print(corr_fano_pcs_total)\n",
    "bf_fano_pcs_total = pingouin.bayesfactor_pearson(corr_fano_pcs_total[0][1], len(x))\n",
    "pcs_total_bf.append(bf_fano_pcs_total)\n",
    "print(bf_fano_pcs_total)\n",
    "\n",
    "print('-----')\n",
    "print('PE')\n",
    "entropy_pcs_total = zip(entropy_s2, pcs_total)\n",
    "x = []; y=[]\n",
    "for point in list(entropy_pcs_total):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_entropy_pcs_total = np.corrcoef(x,y)\n",
    "print(corr_entropy_pcs_total)\n",
    "bf_entropy_pcs_total = pingouin.bayesfactor_pearson(corr_entropy_pcs_total[0][1], len(x))\n",
    "pcs_total_bf.append(bf_entropy_pcs_total)\n",
    "print(bf_entropy_pcs_total)\n",
    "\n",
    "print('-----')\n",
    "print('SampEn')\n",
    "sample_entropy_pcs_total = zip(sample_entropy_s2, pcs_total)\n",
    "x = []; y=[]\n",
    "for point in list(sample_entropy_pcs_total):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_sample_entropy_pcs_total = np.corrcoef(x,y)\n",
    "print(corr_sample_entropy_pcs_total)\n",
    "bf_sample_entropy_pcs_total = pingouin.bayesfactor_pearson(corr_sample_entropy_pcs_total[0][1], len(x))\n",
    "pcs_total_bf.append(bf_sample_entropy_pcs_total)\n",
    "print(bf_sample_entropy_pcs_total)\n",
    "print('-----')\n",
    "print('acf')\n",
    "acf_pcs_total = zip(acf_s2, pcs_total)\n",
    "x = []; y=[]\n",
    "for point in list(acf_pcs_total):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_acf_pcs_total = np.corrcoef(x,y)\n",
    "print(corr_acf_pcs_total)\n",
    "bf_acf_pcs_total = pingouin.bayesfactor_pearson(corr_acf_pcs_total[0][1], len(x))\n",
    "pcs_total_bf.append(bf_acf_pcs_total)\n",
    "print(bf_acf_pcs_total)\n",
    "print('-----')\n",
    "print('smape')\n",
    "smape_pcs_total = zip(smape_s2, pcs_total)\n",
    "x = []; y=[]\n",
    "for point in list(smape_pcs_total):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_smape_pcs_total = np.corrcoef(x,y)\n",
    "print(corr_smape_pcs_total)\n",
    "bf_smape_pcs_total = pingouin.bayesfactor_pearson(corr_smape_pcs_total[0][1], len(x))\n",
    "pcs_total_bf.append(bf_smape_pcs_total)\n",
    "print(bf_smape_pcs_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_rum = df_s2_normal['PCS Rumination Score'].values\n",
    "print('mean')\n",
    "mean_pcs_rum = zip(mean_s2, pcs_rum)\n",
    "x = []; y=[]\n",
    "for point in list(mean_pcs_rum):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_mean_pcs_rum = np.corrcoef(x,y)\n",
    "print(corr_mean_pcs_rum)\n",
    "pcs_rum_bf = []\n",
    "bf_mean_pcs_rum = pingouin.bayesfactor_pearson(corr_mean_pcs_rum[0][1], len(x))\n",
    "pcs_rum_bf.append(bf_mean_pcs_rum)\n",
    "print(bf_mean_pcs_rum)\n",
    "print('-----')\n",
    "print('variance')\n",
    "var_pcs_rum = zip(var_s2, pcs_rum)\n",
    "x = []; y=[]\n",
    "for point in list(var_pcs_rum):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_var_pcs_rum = np.corrcoef(x,y)\n",
    "print(corr_var_pcs_rum)\n",
    "bf_var_pcs_rum = pingouin.bayesfactor_pearson(corr_var_pcs_rum[0][1], len(x))\n",
    "pcs_rum_bf.append(bf_var_pcs_rum)\n",
    "print(bf_var_pcs_rum)\n",
    "\n",
    "print('-----')\n",
    "print('CV')\n",
    "fano_pcs_rum = zip(cv_s2, pcs_rum)\n",
    "x = []; y=[]\n",
    "for point in list(fano_pcs_rum):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_fano_pcs_rum = np.corrcoef(x,y)\n",
    "print(corr_fano_pcs_rum)\n",
    "bf_fano_pcs_rum = pingouin.bayesfactor_pearson(corr_fano_pcs_rum[0][1], len(x))\n",
    "pcs_rum_bf.append(bf_fano_pcs_rum)\n",
    "print(bf_fano_pcs_rum)\n",
    "\n",
    "print('-----')\n",
    "print('PE')\n",
    "entropy_pcs_rum = zip(entropy_s2, pcs_rum)\n",
    "x = []; y=[]\n",
    "for point in list(entropy_pcs_rum):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_entropy_pcs_rum = np.corrcoef(x,y)\n",
    "print(corr_entropy_pcs_rum)\n",
    "bf_entropy_pcs_rum = pingouin.bayesfactor_pearson(corr_entropy_pcs_rum[0][1], len(x))\n",
    "pcs_rum_bf.append(bf_entropy_pcs_rum)\n",
    "print(bf_entropy_pcs_rum)\n",
    "\n",
    "print('-----')\n",
    "print('SampEn')\n",
    "sample_entropy_pcs_rum = zip(sample_entropy_s2, pcs_rum)\n",
    "x = []; y=[]\n",
    "for point in list(sample_entropy_pcs_rum):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_sample_entropy_pcs_rum = np.corrcoef(x,y)\n",
    "print(corr_sample_entropy_pcs_rum)\n",
    "bf_sample_entropy_pcs_rum = pingouin.bayesfactor_pearson(corr_sample_entropy_pcs_rum[0][1], len(x))\n",
    "pcs_rum_bf.append(bf_sample_entropy_pcs_rum)\n",
    "print(bf_sample_entropy_pcs_rum)\n",
    "\n",
    "print('-----')\n",
    "print('acf')\n",
    "acf_pcs_rum = zip(acf_s2, pcs_rum)\n",
    "x = []; y=[]\n",
    "for point in list(acf_pcs_rum):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_acf_pcs_rum = np.corrcoef(x,y)\n",
    "print(corr_acf_pcs_rum)\n",
    "bf_acf_pcs_rum = pingouin.bayesfactor_pearson(corr_acf_pcs_rum[0][1], len(x))\n",
    "pcs_rum_bf.append(bf_acf_pcs_rum)\n",
    "print(bf_acf_pcs_rum)\n",
    "\n",
    "print('-----')\n",
    "print('smape')\n",
    "smape_pcs_rum = zip(smape_s2, pcs_rum)\n",
    "x = []; y=[]\n",
    "for point in list(smape_pcs_rum):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_smape_pcs_rum = np.corrcoef(x,y)\n",
    "print(corr_smape_pcs_rum)\n",
    "bf_smape_pcs_rum = pingouin.bayesfactor_pearson(corr_smape_pcs_rum[0][1], len(x))\n",
    "pcs_rum_bf.append(bf_smape_pcs_rum)\n",
    "print(bf_smape_pcs_rum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_mag = df_s2_normal['PCS Magnification Score'].values\n",
    "print('mean')\n",
    "mean_pcs_mag = zip(mean_s2, pcs_mag)\n",
    "x = []; y=[]\n",
    "for point in list(mean_pcs_mag):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_mean_pcs_mag = np.corrcoef(x,y)\n",
    "print(corr_mean_pcs_mag)\n",
    "pcs_mag_bf = []\n",
    "bf_mean_pcs_mag = pingouin.bayesfactor_pearson(corr_mean_pcs_mag[0][1], len(x))\n",
    "pcs_mag_bf.append(bf_mean_pcs_mag)\n",
    "print(bf_mean_pcs_mag)\n",
    "print('-----')\n",
    "print('variance')\n",
    "var_pcs_mag = zip(var_s2, pcs_mag)\n",
    "x = []; y=[]\n",
    "for point in list(var_pcs_mag):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_var_pcs_mag = np.corrcoef(x,y)\n",
    "print(corr_var_pcs_mag)\n",
    "bf_var_pcs_mag = pingouin.bayesfactor_pearson(corr_var_pcs_mag[0][1], len(x))\n",
    "pcs_mag_bf.append(bf_var_pcs_mag)\n",
    "print(bf_var_pcs_mag)\n",
    "\n",
    "print('-----')\n",
    "print('CV')\n",
    "fano_pcs_mag = zip(cv_s2, pcs_mag)\n",
    "x = []; y=[]\n",
    "for point in list(fano_pcs_mag):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_fano_pcs_mag = np.corrcoef(x,y)\n",
    "print(corr_fano_pcs_mag)\n",
    "bf_fano_pcs_mag = pingouin.bayesfactor_pearson(corr_fano_pcs_mag[0][1], len(x))\n",
    "pcs_mag_bf.append(bf_fano_pcs_mag)\n",
    "print(bf_fano_pcs_mag)\n",
    "\n",
    "print('-----')\n",
    "print('PE')\n",
    "entropy_pcs_mag = zip(entropy_s2, pcs_mag)\n",
    "x = []; y=[]\n",
    "for point in list(entropy_pcs_mag):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_entropy_pcs_mag = np.corrcoef(x,y)\n",
    "print(corr_entropy_pcs_mag)\n",
    "bf_entropy_pcs_mag = pingouin.bayesfactor_pearson(corr_entropy_pcs_mag[0][1], len(x))\n",
    "pcs_mag_bf.append(bf_entropy_pcs_mag)\n",
    "print(bf_entropy_pcs_mag)\n",
    "\n",
    "print('-----')\n",
    "print('SampEn')\n",
    "sample_entropy_pcs_mag = zip(sample_entropy_s2, pcs_mag)\n",
    "x = []; y=[]\n",
    "for point in list(sample_entropy_pcs_mag):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_sample_entropy_pcs_mag = np.corrcoef(x,y)\n",
    "print(corr_sample_entropy_pcs_mag)\n",
    "bf_sample_entropy_pcs_mag = pingouin.bayesfactor_pearson(corr_sample_entropy_pcs_mag[0][1], len(x))\n",
    "pcs_mag_bf.append(bf_sample_entropy_pcs_mag)\n",
    "print(bf_sample_entropy_pcs_mag)\n",
    "\n",
    "print('-----')\n",
    "print('acf')\n",
    "acf_pcs_mag = zip(acf_s2, pcs_mag)\n",
    "x = []; y=[]\n",
    "for point in list(acf_pcs_mag):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_acf_pcs_mag = np.corrcoef(x,y)\n",
    "print(corr_acf_pcs_mag)\n",
    "bf_acf_pcs_mag = pingouin.bayesfactor_pearson(corr_acf_pcs_mag[0][1], len(x))\n",
    "pcs_mag_bf.append(bf_acf_pcs_mag)\n",
    "print(bf_acf_pcs_mag)\n",
    "print('-----')\n",
    "print('smape')\n",
    "smape_pcs_mag = zip(smape_s2, pcs_mag)\n",
    "x = []; y=[]\n",
    "for point in list(smape_pcs_mag):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_smape_pcs_mag = np.corrcoef(x,y)\n",
    "print(corr_smape_pcs_mag)\n",
    "bf_smape_pcs_mag = pingouin.bayesfactor_pearson(corr_smape_pcs_mag[0][1], len(x))\n",
    "pcs_mag_bf.append(bf_smape_pcs_mag)\n",
    "print(bf_smape_pcs_mag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_help = df_s2_normal['PCS Helplessness Score'].values\n",
    "print('mean')\n",
    "mean_pcs_help = zip(mean_s2, pcs_help)\n",
    "x = []; y=[]\n",
    "for point in list(mean_pcs_help):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_mean_pcs_help = np.corrcoef(x,y)\n",
    "print(corr_mean_pcs_help)\n",
    "pcs_help_bf = []\n",
    "bf_mean_pcs_help = pingouin.bayesfactor_pearson(corr_mean_pcs_help[0][1], len(x))\n",
    "pcs_help_bf.append(bf_mean_pcs_help)\n",
    "print(bf_mean_pcs_help)\n",
    "print('-----')\n",
    "print('variance')\n",
    "var_pcs_help = zip(var_s2, pcs_help)\n",
    "x = []; y=[]\n",
    "for point in list(var_pcs_help):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_var_pcs_help = np.corrcoef(x,y)\n",
    "print(corr_var_pcs_help)\n",
    "bf_var_pcs_help = pingouin.bayesfactor_pearson(corr_var_pcs_help[0][1], len(x))\n",
    "pcs_help_bf.append(bf_var_pcs_help)\n",
    "print(bf_var_pcs_help)\n",
    "\n",
    "print('-----')\n",
    "print('CV')\n",
    "fano_pcs_help = zip(cv_s2, pcs_help)\n",
    "x = []; y=[]\n",
    "for point in list(fano_pcs_help):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_fano_pcs_help = np.corrcoef(x,y)\n",
    "print(corr_fano_pcs_help)\n",
    "bf_fano_pcs_help = pingouin.bayesfactor_pearson(corr_fano_pcs_help[0][1], len(x))\n",
    "pcs_help_bf.append(bf_fano_pcs_help)\n",
    "print(bf_fano_pcs_help)\n",
    "\n",
    "print('-----')\n",
    "print('PE')\n",
    "entropy_pcs_help = zip(entropy_s2, pcs_help)\n",
    "x = []; y=[]\n",
    "for point in list(entropy_pcs_help):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_entropy_pcs_help = np.corrcoef(x,y)\n",
    "print(corr_entropy_pcs_help)\n",
    "bf_entropy_pcs_help = pingouin.bayesfactor_pearson(corr_entropy_pcs_help[0][1], len(x))\n",
    "pcs_help_bf.append(bf_entropy_pcs_help)\n",
    "print(bf_entropy_pcs_help)\n",
    "\n",
    "print('-----')\n",
    "print('SampEn')\n",
    "sample_entropy_pcs_help = zip(sample_entropy_s2, pcs_help)\n",
    "x = []; y=[]\n",
    "for point in list(sample_entropy_pcs_help):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_sample_entropy_pcs_help = np.corrcoef(x,y)\n",
    "print(corr_sample_entropy_pcs_help)\n",
    "bf_sample_entropy_pcs_help = pingouin.bayesfactor_pearson(corr_sample_entropy_pcs_help[0][1], len(x))\n",
    "pcs_help_bf.append(bf_sample_entropy_pcs_help)\n",
    "print(bf_sample_entropy_pcs_help)\n",
    "\n",
    "print('-----')\n",
    "print('acf')\n",
    "acf_pcs_help = zip(acf_s2, pcs_help)\n",
    "x = []; y=[]\n",
    "for point in list(acf_pcs_help):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_acf_pcs_help = np.corrcoef(x,y)\n",
    "print(corr_acf_pcs_help)\n",
    "bf_acf_pcs_help = pingouin.bayesfactor_pearson(corr_acf_pcs_help[0][1], len(x))\n",
    "pcs_help_bf.append(bf_acf_pcs_help)\n",
    "print(bf_acf_pcs_help)\n",
    "print('-----')\n",
    "print('smape')\n",
    "smape_pcs_help = zip(smape_s2, pcs_help)\n",
    "x = []; y=[]\n",
    "for point in list(smape_pcs_help):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_smape_pcs_help = np.corrcoef(x,y)\n",
    "print(corr_smape_pcs_help)\n",
    "bf_smape_pcs_help = pingouin.bayesfactor_pearson(corr_smape_pcs_help[0][1], len(x))\n",
    "pcs_help_bf.append(bf_smape_pcs_help)\n",
    "print(bf_smape_pcs_help)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gad7_total = df_s2_normal['Total GAD7 Score'].values\n",
    "print('mean')\n",
    "mean_gad7 = zip(mean_s2, gad7_total)\n",
    "x = []; y=[]\n",
    "for point in list(mean_gad7):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_mean_gad7 = np.corrcoef(x,y)\n",
    "print(corr_mean_gad7)\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "gad7_bf = []\n",
    "bf_mean_gad7 = pingouin.bayesfactor_pearson(corr_mean_gad7[0][1], len(x))\n",
    "gad7_bf.append(bf_mean_gad7)\n",
    "print(bf_mean_gad7)\n",
    "print('-----')\n",
    "print('variance')\n",
    "var_gad7 = zip(var_s2, gad7_total)\n",
    "x = []; y=[]\n",
    "for point in list(var_gad7):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_var_gad7 = np.corrcoef(x,y)\n",
    "print(corr_var_gad7)\n",
    "bf_var_gad7 = pingouin.bayesfactor_pearson(corr_var_gad7[0][1], len(x))\n",
    "gad7_bf.append(bf_var_gad7)\n",
    "print(bf_var_gad7)\n",
    "\n",
    "print('-----')\n",
    "print('CV')\n",
    "fano_gad7 = zip(cv_s2, gad7_total)\n",
    "x = []; y=[]\n",
    "for point in list(fano_gad7):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_fano_gad7 = np.corrcoef(x,y)\n",
    "print(corr_fano_gad7)\n",
    "bf_fano_gad7 = pingouin.bayesfactor_pearson(corr_fano_gad7[0][1], len(x))\n",
    "gad7_bf.append(bf_fano_gad7)\n",
    "print(bf_fano_gad7)\n",
    "\n",
    "print('-----')\n",
    "print('PE')\n",
    "entropy_gad7 = zip(entropy_s2, gad7_total)\n",
    "x = []; y=[]\n",
    "for point in list(entropy_gad7):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_entropy_gad7 = np.corrcoef(x,y)\n",
    "print(corr_entropy_gad7)\n",
    "bf_entropy_gad7 = pingouin.bayesfactor_pearson(corr_entropy_gad7[0][1], len(x))\n",
    "gad7_bf.append(bf_entropy_gad7)\n",
    "print(bf_entropy_gad7)\n",
    "\n",
    "print('-----')\n",
    "print('SampEn')\n",
    "sample_entropy_gad7 = zip(sample_entropy_s2, gad7_total)\n",
    "x = []; y=[]\n",
    "for point in list(sample_entropy_gad7):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_sample_entropy_gad7 = np.corrcoef(x,y)\n",
    "print(corr_sample_entropy_gad7)\n",
    "bf_sample_entropy_gad7 = pingouin.bayesfactor_pearson(corr_sample_entropy_gad7[0][1], len(x))\n",
    "gad7_bf.append(bf_sample_entropy_gad7)\n",
    "print(bf_sample_entropy_gad7)\n",
    "\n",
    "print('-----')\n",
    "print('acf')\n",
    "acf_gad7 = zip(acf_s2, gad7_total)\n",
    "x = []; y=[]\n",
    "for point in list(acf_gad7):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_acf_gad7 = np.corrcoef(x,y)\n",
    "print(corr_acf_gad7)\n",
    "bf_acf_gad7 = pingouin.bayesfactor_pearson(corr_acf_gad7[0][1], len(x))\n",
    "gad7_bf.append(bf_acf_gad7)\n",
    "print(bf_acf_gad7)\n",
    "print('-----')\n",
    "print('smape')\n",
    "smape_gad7 = zip(smape_s2, gad7_total)\n",
    "x = []; y=[]\n",
    "for point in list(smape_gad7):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_smape_gad7 = np.corrcoef(x,y)\n",
    "print(corr_smape_gad7)\n",
    "bf_smape_gad7 = pingouin.bayesfactor_pearson(corr_smape_gad7[0][1], len(x))\n",
    "gad7_bf.append(bf_smape_gad7)\n",
    "print(bf_smape_gad7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phq9_total = df_s2_normal['Total PHQ9 Score'].values\n",
    "print('mean')\n",
    "mean_phq9 = zip(mean_s2, phq9_total)\n",
    "x = []; y=[]\n",
    "for point in list(mean_phq9):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_mean_phq9 = np.corrcoef(x,y)\n",
    "print(corr_mean_phq9)\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "phq9_bf = []\n",
    "bf_mean_phq9 = pingouin.bayesfactor_pearson(corr_mean_phq9[0][1], len(x))\n",
    "phq9_bf.append(bf_mean_phq9)\n",
    "print(bf_mean_phq9)\n",
    "print('-----')\n",
    "print('variance')\n",
    "var_phq9 = zip(var_s2, phq9_total)\n",
    "x = []; y=[]\n",
    "for point in list(var_phq9):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_var_phq9 = np.corrcoef(x,y)\n",
    "print(corr_var_phq9)\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "bf_var_phq9 = pingouin.bayesfactor_pearson(corr_var_phq9[0][1], len(x))\n",
    "phq9_bf.append(bf_var_phq9)\n",
    "print(bf_var_phq9)\n",
    "\n",
    "print('-----')\n",
    "print('CV')\n",
    "fano_phq9 = zip(cv_s2, phq9_total)\n",
    "x = []; y=[]\n",
    "for point in list(fano_phq9):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_fano_phq9 = np.corrcoef(x,y)\n",
    "bf_fano_phq9 = pingouin.bayesfactor_pearson(corr_fano_phq9[0][1], len(x))\n",
    "phq9_bf.append(bf_fano_phq9)\n",
    "print(corr_fano_phq9)\n",
    "print(bf_fano_phq9)\n",
    "\n",
    "\n",
    "print('-----')\n",
    "print('PE')\n",
    "entropy_phq9 = zip(entropy_s2, phq9_total)\n",
    "x = []; y=[]\n",
    "for point in list(entropy_phq9):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_entropy_phq9 = np.corrcoef(x,y)\n",
    "print(corr_entropy_phq9)\n",
    "bf_entropy_phq9 = pingouin.bayesfactor_pearson(corr_entropy_phq9[0][1], len(x))\n",
    "phq9_bf.append(bf_entropy_phq9)\n",
    "print(bf_entropy_phq9)\n",
    "\n",
    "print('-----')\n",
    "print('SampEn')\n",
    "sample_entropy_phq9 = zip(sample_entropy_s2, phq9_total)\n",
    "x = []; y=[]\n",
    "for point in list(sample_entropy_phq9):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_sample_entropy_phq9 = np.corrcoef(x,y)\n",
    "print(corr_sample_entropy_phq9)\n",
    "bf_sample_entropy_phq9 = pingouin.bayesfactor_pearson(corr_sample_entropy_phq9[0][1], len(x))\n",
    "phq9_bf.append(bf_sample_entropy_phq9)\n",
    "print(bf_sample_entropy_phq9)\n",
    "\n",
    "print('-----')\n",
    "print('acf')\n",
    "acf_phq9 = zip(acf_s2, phq9_total)\n",
    "x = []; y=[]\n",
    "for point in list(acf_phq9):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_acf_phq9 = np.corrcoef(x,y)\n",
    "print(corr_acf_phq9)\n",
    "bf_acf_phq9 = pingouin.bayesfactor_pearson(corr_acf_phq9[0][1], len(x))\n",
    "phq9_bf.append(bf_acf_phq9)\n",
    "print(bf_acf_phq9)\n",
    "\n",
    "print('-----')\n",
    "print('smape')\n",
    "smape_phq9 = zip(smape_s2, phq9_total)\n",
    "x = []; y=[]\n",
    "for point in list(smape_phq9):\n",
    "   x.append(point[0])\n",
    "   y.append(point[1])\n",
    "for i in range(len(y)):\n",
    "   if pd.isnull(y[i]):\n",
    "      x[i] = np.NaN\n",
    "   if pd.isnull(x[i]):\n",
    "      y[i] = np.NaN\n",
    "x = pd.Series(x).dropna().to_list()\n",
    "y = pd.Series(y).dropna().to_list()\n",
    "corr_smape_phq9 = np.corrcoef(x,y)\n",
    "print(corr_smape_phq9)\n",
    "bf_smape_phq9 = pingouin.bayesfactor_pearson(corr_smape_phq9[0][1], len(x))\n",
    "phq9_bf.append(bf_smape_phq9)\n",
    "print(bf_smape_phq9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat map code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14,14)\n",
    "plt.rcParams['font.size'] = '14'\n",
    "total_bf = np.vstack([msk_bf, msk_lastq_bf, bpisev_bf, bpiint_bf, start_bf, startsub_bf, pcs_total_bf, pcs_rum_bf, pcs_mag_bf, pcs_help_bf, gad7_bf, phq9_bf])\n",
    "sns.heatmap(total_bf, norm=LogNorm(), xticklabels=['Mean', 'Variance', 'CV','PE', 'SampEn','ACF time scale', 'SMAPE', 'Fractal dimension'], cmap=\"YlGnBu\", annot = True,\n",
    "            yticklabels = ['MSK score', 'Number of Physically Active Days','BPI mean severity', 'BPI mean interference', 'total STarT', 'STarT sub Score', 'Total PCS score', 'PCS rumination score', 'PCS magnification score', 'PCS helplessness score', 'Total GAD7', 'Total PHQ9'])\n",
    "plt.title('Heat map with BF10 values of questionnaires & various features')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-retest reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d1 = pd.read_pickle('../Data/descriptive_data/DescriptiveData.pkl')\n",
    "data_d2 = pd.read_pickle('../Data/data_file/D2_puregooddata_split_31032023.pkl')\n",
    "participant_list_d2 = data_d2['PID'].unique()\n",
    "data_d1_in_d2 = data_d1[data_d1['PID'].isin(participant_list_d2)]\n",
    "mean_overall_d1 = data_d1_in_d2['mean overall'].values\n",
    "var_overall_d1 = data_d1_in_d2['variance overall'].values\n",
    "cv_overall_d1 = data_d1_in_d2['CV overall'].values\n",
    "pe_overall_d1 = data_d1_in_d2['permutation entropy overall'].values\n",
    "sampen_overall_d1 = data_d1_in_d2['sample entropy overall'].values\n",
    "fractal_overall_d1 = data_d1_in_d2['fractal dimension overall'].values\n",
    "mean_all_d1 = data_d1_in_d2['mean'].values\n",
    "var_all_d1 = data_d1_in_d2['variance'].values\n",
    "cv_all_d1 = data_d1_in_d2['CV'].values\n",
    "pe_all_d1 = data_d1_in_d2['permutation entropy'].values\n",
    "sampen_all_d1 = data_d1_in_d2['sample entropy'].values\n",
    "fractal_all_d1 = data_d1_in_d2['fractal dimension'].values\n",
    "data_d2 = pd.read_pickle('../Data/descriptive_data/DescriptiveDataD2.pkl')\n",
    "mean_overall_d2 = data_d2['mean overall'].values\n",
    "var_overall_d2 = data_d2['variance overall'].values\n",
    "cv_overall_d2 = data_d2['CV overall'].values\n",
    "pe_overall_d2 = data_d2['permutation entropy overall'].values\n",
    "sampen_overall_d2 = data_d2['sample entropy overall'].values\n",
    "fractal_overall_d2 = data_d2['fractal dimension overall'].values\n",
    "mean_all_d2 = data_d2['mean'].values\n",
    "var_all_d2 = data_d2['variance'].values\n",
    "cv_all_d2 = data_d2['CV'].values\n",
    "pe_all_d2 = data_d2['permutation entropy'].values\n",
    "sampen_all_d2 = data_d2['sample entropy'].values\n",
    "fractal_all_d2 = data_d2['fractal dimension'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson coefficient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pearson\\'s Coefficient')\n",
    "print('-----')\n",
    "print('mean')\n",
    "print(np.corrcoef(mean_overall_d1, mean_overall_d2))\n",
    "print('-----')\n",
    "print('variance')\n",
    "print(np.corrcoef(var_overall_d1, var_overall_d2))\n",
    "print('-----')\n",
    "print('CV')\n",
    "print(np.corrcoef(cv_overall_d1, cv_overall_d2))\n",
    "print('-----')\n",
    "print('PE')\n",
    "print(np.corrcoef(pe_overall_d1, pe_overall_d2))\n",
    "print('-----')\n",
    "print('SampEn')\n",
    "print(np.corrcoef(sampen_overall_d1, sampen_overall_d2))\n",
    "print('-----')\n",
    "print('fractal')\n",
    "print(np.corrcoef(fractal_overall_d1, fractal_overall_d2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICC method\n",
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_1_mean_df = pd.DataFrame({'mean': mean_overall_d1, 'participant': range(len(mean_overall_d1)), 'session': np.zeros(len(mean_overall_d1))})\n",
    "session_2_mean_df = pd.DataFrame({'mean': mean_overall_d2, 'participant': range(len(mean_overall_d2)), 'session': np.ones(len(mean_overall_d2))})\n",
    "mean_df = pd.concat([session_1_mean_df,session_2_mean_df])\n",
    "icc_mean_overall = pg.intraclass_corr(data=mean_df, targets = 'participant', raters = 'session', ratings='mean')\n",
    "icc_mean_overall.set_index('Type')\n",
    "mean_all_sep_d1 = []\n",
    "mean_all_participant_d1 = []\n",
    "for i in range(len(mean_all_d1)):\n",
    "    for j in range(len(mean_all_d1[i])):\n",
    "        mean_all_sep_d1.append(mean_all_d1[i][j])\n",
    "        mean_all_participant_d1.append(i)\n",
    "mean_all_sep_d2 = []\n",
    "mean_all_participant_d2 = []\n",
    "for i in range(len(mean_all_d2)):\n",
    "    for j in range(len(mean_all_d2[i])):\n",
    "        mean_all_sep_d2.append(mean_all_d2[i][j])\n",
    "        mean_all_participant_d2.append(i)\n",
    "session_1_mean_df_all = pd.DataFrame({'mean': mean_all_sep_d1, 'participant': mean_all_participant_d1, 'session': np.zeros(len(mean_all_sep_d1))})\n",
    "session_2_mean_df_all = pd.DataFrame({'mean': mean_all_sep_d2, 'participant': mean_all_participant_d2, 'session': np.ones(len(mean_all_sep_d2))})\n",
    "mean_df_all = pd.concat([session_1_mean_df_all,session_2_mean_df_all])\n",
    "icc_mean_all = pg.intraclass_corr(data=mean_df_all, targets = 'participant', raters = 'session', ratings='mean')\n",
    "icc_mean_all.set_index('Type')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_1_var_df = pd.DataFrame({'var': var_overall_d1, 'participant': range(len(var_overall_d1)), 'session': np.zeros(len(var_overall_d1))})\n",
    "session_2_var_df = pd.DataFrame({'var': var_overall_d2, 'participant': range(len(var_overall_d2)), 'session': np.ones(len(var_overall_d2))})\n",
    "var_df = pd.concat([session_1_var_df,session_2_var_df])\n",
    "icc_var_overall = pg.intraclass_corr(data=var_df, targets = 'participant', raters = 'session', ratings='var')\n",
    "icc_var_overall.set_index('Type')\n",
    "var_all_sep_d1 = []\n",
    "var_all_participant_d1 = []\n",
    "for i in range(len(var_all_d1)):\n",
    "    for j in range(len(var_all_d1[i])):\n",
    "        var_all_sep_d1.append(var_all_d1[i][j])\n",
    "        var_all_participant_d1.append(i)\n",
    "var_all_sep_d2 = []\n",
    "var_all_participant_d2 = []\n",
    "for i in range(len(var_all_d2)):\n",
    "    for j in range(len(var_all_d2[i])):\n",
    "        var_all_sep_d2.append(var_all_d2[i][j])\n",
    "        var_all_participant_d2.append(i)\n",
    "session_1_var_df_all = pd.DataFrame({'var': var_all_sep_d1, 'participant': var_all_participant_d1, 'session': np.zeros(len(var_all_sep_d1))})\n",
    "session_2_var_df_all = pd.DataFrame({'var': var_all_sep_d2, 'participant': var_all_participant_d2, 'session': np.ones(len(var_all_sep_d2))})\n",
    "var_df_all = pd.concat([session_1_var_df_all,session_2_var_df_all])\n",
    "icc_var_all = pg.intraclass_corr(data=var_df_all, targets = 'participant', raters = 'session', ratings='var')\n",
    "icc_var_all.set_index('Type')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_1_cv_df = pd.DataFrame({'cv': cv_overall_d1, 'participant': range(len(cv_overall_d1)), 'session': np.zeros(len(cv_overall_d1))})\n",
    "session_2_cv_df = pd.DataFrame({'cv': cv_overall_d2, 'participant': range(len(cv_overall_d2)), 'session': np.ones(len(cv_overall_d2))})\n",
    "cv_df = pd.concat([session_1_cv_df,session_2_cv_df])\n",
    "icc_cv_overall = pg.intraclass_corr(data=cv_df, targets = 'participant', raters = 'session', ratings='cv')\n",
    "icc_cv_overall.set_index('Type')\n",
    "cv_all_sep_d1 = []\n",
    "cv_all_participant_d1 = []\n",
    "for i in range(len(cv_all_d1)):\n",
    "    for j in range(len(cv_all_d1[i])):\n",
    "        cv_all_sep_d1.append(cv_all_d1[i][j])\n",
    "        cv_all_participant_d1.append(i)\n",
    "cv_all_sep_d2 = []\n",
    "cv_all_participant_d2 = []\n",
    "for i in range(len(cv_all_d2)):\n",
    "    for j in range(len(cv_all_d2[i])):\n",
    "        cv_all_sep_d2.append(cv_all_d2[i][j])\n",
    "        cv_all_participant_d2.append(i)\n",
    "session_1_cv_df_all = pd.DataFrame({'cv': cv_all_sep_d1, 'participant': cv_all_participant_d1, 'session': np.zeros(len(cv_all_sep_d1))})\n",
    "session_2_cv_df_all = pd.DataFrame({'cv': cv_all_sep_d2, 'participant': cv_all_participant_d2, 'session': np.ones(len(cv_all_sep_d2))})\n",
    "cv_df_all = pd.concat([session_1_cv_df_all,session_2_cv_df_all])\n",
    "icc_cv_all = pg.intraclass_corr(data=cv_df_all, targets = 'participant', raters = 'session', ratings='cv')\n",
    "icc_cv_all.set_index('Type')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_1_pe_df = pd.DataFrame({'pe': pe_overall_d1, 'participant': range(len(pe_overall_d1)), 'session': np.zeros(len(pe_overall_d1))})\n",
    "session_2_pe_df = pd.DataFrame({'pe': pe_overall_d2, 'participant': range(len(pe_overall_d2)), 'session': np.ones(len(pe_overall_d2))})\n",
    "pe_df = pd.concat([session_1_pe_df,session_2_pe_df])\n",
    "icc_pe_overall = pg.intraclass_corr(data=pe_df, targets = 'participant', raters = 'session', ratings='pe')\n",
    "icc_pe_overall.set_index('Type')\n",
    "pe_all_sep_d1 = []\n",
    "pe_all_participant_d1 = []\n",
    "for i in range(len(pe_all_d1)):\n",
    "    for j in range(len(pe_all_d1[i])):\n",
    "        pe_all_sep_d1.append(pe_all_d1[i][j])\n",
    "        pe_all_participant_d1.append(i)\n",
    "pe_all_sep_d2 = []\n",
    "pe_all_participant_d2 = []\n",
    "for i in range(len(pe_all_d2)):\n",
    "    for j in range(len(pe_all_d2[i])):\n",
    "        pe_all_sep_d2.append(pe_all_d2[i][j])\n",
    "        pe_all_participant_d2.append(i)\n",
    "session_1_pe_df_all = pd.DataFrame({'pe': pe_all_sep_d1, 'participant': pe_all_participant_d1, 'session': np.zeros(len(pe_all_sep_d1))})\n",
    "session_2_pe_df_all = pd.DataFrame({'pe': pe_all_sep_d2, 'participant': pe_all_participant_d2, 'session': np.ones(len(pe_all_sep_d2))})\n",
    "pe_df_all = pd.concat([session_1_pe_df_all,session_2_pe_df_all])\n",
    "icc_pe_all = pg.intraclass_corr(data=pe_df_all, targets = 'participant', raters = 'session', ratings='pe')\n",
    "icc_pe_all.set_index('Type')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SampEn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_1_sampen_df = pd.DataFrame({'sampen': sampen_overall_d1, 'participant': range(len(sampen_overall_d1)), 'session': np.zeros(len(sampen_overall_d1))})\n",
    "session_2_sampen_df = pd.DataFrame({'sampen': sampen_overall_d2, 'participant': range(len(sampen_overall_d2)), 'session': np.ones(len(sampen_overall_d2))})\n",
    "sampen_df = pd.concat([session_1_sampen_df,session_2_sampen_df])\n",
    "icc_sampen_overall = pg.intraclass_corr(data=sampen_df, targets = 'participant', raters = 'session', ratings='sampen')\n",
    "icc_sampen_overall.set_index('Type')\n",
    "sampen_all_sep_d1 = []\n",
    "sampen_all_participant_d1 = []\n",
    "for i in range(len(sampen_all_d1)):\n",
    "    for j in range(len(sampen_all_d1[i])):\n",
    "        sampen_all_sep_d1.append(sampen_all_d1[i][j])\n",
    "        sampen_all_participant_d1.append(i)\n",
    "sampen_all_sep_d2 = []\n",
    "sampen_all_participant_d2 = []\n",
    "for i in range(len(sampen_all_d2)):\n",
    "    for j in range(len(sampen_all_d2[i])):\n",
    "        sampen_all_sep_d2.append(sampen_all_d2[i][j])\n",
    "        sampen_all_participant_d2.append(i)\n",
    "session_1_sampen_df_all = pd.DataFrame({'sampen': sampen_all_sep_d1, 'participant': sampen_all_participant_d1, 'session': np.zeros(len(sampen_all_sep_d1))})\n",
    "session_2_sampen_df_all = pd.DataFrame({'sampen': sampen_all_sep_d2, 'participant': sampen_all_participant_d2, 'session': np.ones(len(sampen_all_sep_d2))})\n",
    "sampen_df_all = pd.concat([session_1_sampen_df_all,session_2_sampen_df_all])\n",
    "icc_sampen_all = pg.intraclass_corr(data=sampen_df_all, targets = 'participant', raters = 'session', ratings='sampen')\n",
    "icc_sampen_all.set_index('Type')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fractal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_1_fractal_df = pd.DataFrame({'fractal': fractal_overall_d1, 'participant': range(len(fractal_overall_d1)), 'session': np.zeros(len(fractal_overall_d1))})\n",
    "session_2_fractal_df = pd.DataFrame({'fractal': fractal_overall_d2, 'participant': range(len(fractal_overall_d2)), 'session': np.ones(len(fractal_overall_d2))})\n",
    "fractal_df = pd.concat([session_1_fractal_df,session_2_fractal_df])\n",
    "icc_fractal_overall = pg.intraclass_corr(data=fractal_df, targets = 'participant', raters = 'session', ratings='fractal')\n",
    "icc_fractal_overall.set_index('Type')\n",
    "fractal_all_sep_d1 = []\n",
    "fractal_all_participant_d1 = []\n",
    "for i in range(len(fractal_all_d1)):\n",
    "    for j in range(len(fractal_all_d1[i])):\n",
    "        fractal_all_sep_d1.append(fractal_all_d1[i][j])\n",
    "        fractal_all_participant_d1.append(i)\n",
    "fractal_all_sep_d2 = []\n",
    "fractal_all_participant_d2 = []\n",
    "for i in range(len(fractal_all_d2)):\n",
    "    for j in range(len(fractal_all_d2[i])):\n",
    "        fractal_all_sep_d2.append(fractal_all_d2[i][j])\n",
    "        fractal_all_participant_d2.append(i)\n",
    "session_1_fractal_df_all = pd.DataFrame({'fractal': fractal_all_sep_d1, 'participant': fractal_all_participant_d1, 'session': np.zeros(len(fractal_all_sep_d1))})\n",
    "session_2_fractal_df_all = pd.DataFrame({'fractal': fractal_all_sep_d2, 'participant': fractal_all_participant_d2, 'session': np.ones(len(fractal_all_sep_d2))})\n",
    "fractal_df_all = pd.concat([session_1_fractal_df_all,session_2_fractal_df_all])\n",
    "icc_fractal_all = pg.intraclass_corr(data=fractal_df_all, targets = 'participant', raters = 'session', ratings='fractal')\n",
    "icc_fractal_all.set_index('Type')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
